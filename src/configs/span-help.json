{
  "config": "config file",
  "name": "model name (Default: 'type-cnn-lstm-crf')",
  "checkpoint_dir": "checkpoints directory (Default: '../../checkpoints')",
  "eval": "only evaluate existing checkpoint model (none/best/<checkpoint-id>) (Default: 'none')",
  "query": "query mode, can be used with eval to work with best model (Default: False)",
  "data_dir": "path to input dataset directory (Default: '../../data/GENIA_term_3.02')",
  "out_dir": "path to output directory (Default: '../../data/GENIA_term_3.02/out')",
  "train_path": "path to train dataset (train.tsv|std_train.tsv|jnlpba_train.tsv) (Default: 'train.tsv')",
  "dev_path": "path to dev dataset (dev.tsv|std_dev.tsv|jnlpba_dev.tsv) (Default: 'dev.tsv')",
  "test_path": "path to test dataset (test.tsv|std_test.tsv|jnlpba_test.tsv) (Default: 'test.tsv')",
  "word_vocab_path": "path to word vocab (Default: 'glove_vocab.txt')",
  "tags_path": "path to output tags vocab. Use 'span_tag_vocab.txt' for full tags vocab.  Use 'std_tag_vocab.txt' for standard 5 tags vocab.  Use 'jnlpba_span_tag_vocab.tsv' for exact (5-tag) settings used by MTL-BioInformatics-2016  (ref: https://github.com/cambridgeltl/MTL-Bioinformatics-2016) Use 'out_freq_tag_vocab.txt' for reduced tags, when considering input tags information.  (Default: 'span_tag_vocab.txt')",
  "out_tag_names_path": "path to output tag general names. Use 'span_tag_names.txt' for full tags vocab names. Use 'std_tag_names.txt' for standard 5 tags vocab names. Use 'jnlpba_span_tag_names.txt' for exact (5-tag) settings used by MTL-BioInformatics-2016 (ref: https://github.com/cambridgeltl/MTL-Bioinformatics-2016)Use 'out_freq_tag_names.txt' for reduced tags, when considering input tags information. (Default: 'span_tag_names.txt')",
  "inp_tag_vocab_path": "path to input tags vocab. Use 'empty_inp_tag_vocab.txt' if don't want to use tag info. Use 'inp_freq_tag_vocab.txt' for specifying default input tag info.(Default: 'empty_inp_tag_vocab.txt')",
  "pos_tag_vocab_path": "path to POS tags vocab. (pos_tag_vocab.txt|jnlpba_pos_tag_vocab.txt) (Default: 'pos_tag_vocab.txt')",
  "dep_tag_vocab_path": "path to dependency-parse tags vocab. (dep_tag_vocab.txt|jnlpba_dep_tag_vocab.txt) (Default: 'dep_tag_vocab.txt')",
  "emb_path": "path to pre-trained word embeddings (Default: '../../../../Embeddings/glove.6B.50d.txt')",
  "tag_emb_path": "path to pre-trained tag embeddings, relative to data_dir (jnlpba_tag_w2v_emb.txt|jnlpba_tag_use_emb.txt|jnlpba_tag_full_emb.txt) (std_tag_w2v_emb.txt|std_tag_use_emb.txt|std_tag_full_emb.txt) (tag_w2v_emb.txt|tag_use_emb.txt|tag_full_emb.txt) (Default: 'tag_w2v_emb.txt')",
  "guidance_dir": "path to guidance input files (Default: '../../data/genia/out/span-cnn-lstm-high')",
  "num_epochs": "# epochs to train (Default: 500)",
  "batch_size": "batch size (Default: 128)",
  "word_emb_dim": "word embedding dimension (Default: 50)",
  "pos_tag_emb_dim": "POS tag embedding dimension (Default: 15)",
  "dep_tag_emb_dim": "dep-parse tag embedding dimension (Default: 15)",
  "max_word_len": "max. #chars in word (Default: 30)",
  "max_seq_len": "max. #words in sentence (Default: 60)",
  "conv1_dim": "conv1 layer output channels (Default: 128)",
  "hidden_dim": "hidden state dim for LSTM, if used (Default: 256)",
  "use_maxpool": "max pool over CNN output to get char embeddings, else does concatenation (Default: False)",
  "use_pos_tag": "embed POS tag information (Default: False)",
  "use_dep_tag": "embed dep-parse tag information (Default: False)",
  "use_tag_cosine_sim": "compute cosine sim with tag embeddings as additional layer in model (Default: False)",
  "kernel_size": "kernel size for CNN (Default: 5)",
  "num_lstm_layers": "no. of LSTM layers (Default: 1)",
  "use_char": "char embedding type (none/lower/all) (Default: 'lower')",
  "use_pattern": "pattern embedding type (none/one-to-one/condensed) (Default: 'condensed')",
  "escape_digits": "replace digits(0-9) with 'd' tag in pattern capturing (Default: False)",
  "ignore_word_lengths": "ignore word lengths in pattern capturing (Default: False)",
  "no_lstm": "don't use LSTM to capture neighbor context. Directly CRF over individual token level CNN (Default: False)",
  "use_tag_info": "type information (none/self/window/pretrained) (Default: 'none')",
  "span_pooling": "how to form span matrix from token matrix (boundary|avg) (Default: 'boundary')",
  "loss_masking": "masking to be used during loss computation (none|pre|post) (Default: 'pre')",
  "use_tfo": "use transformer (may not use LSTM then). 'simple' creates a basic tfo. 'xl' uses TransformerXL model layer. (none|simple|xl) (Default: 'none')",
  "window_size": "size of context window for type info on either side of current token (Default: 5)",
  "use_word": "use word(token) embeddings (none|rand|glove|allenai/scibert_scivocab_uncased|bert-base-uncased|../../../resources/biobert_v1.1_pubmed) (Default: allenai/scibert_scivocab_uncased)",
  "use_pre_padding": "pre-padding for char/word (Default: False)",
  "word_emb_model_from_tf": "word embedding generator model is a pretrained tensorflow model. Use 'True' for models like, '../../../resources/biobert_v1.1_pubmed' (Default: False)",
  "use_class_guidance": "take guidance through pre-trained class embeddings (Default: False)",
  "fine_tune_bert": "fine-tune bert embeddings (Default: False)",
  "lr": "learning rate (Default: 0.001)",
  "dropout_ratio": "dropout ratio (Default: 0.5)",
  "seed": "manual seed for reproducibility (Default: 42)",
  "use_cpu": "force CPU usage (Default: False)",
  "no_eval_print": "don't output verbose evaluation matrices (Default: False)"
}