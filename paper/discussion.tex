From the results of experiments reported in the previous section, we make the following observations:

\begin{itemize}
    \item From Table \ref{tab:main}, we see that \modelname{} outperforms sequence tagging and QA-based baselines on three cross-domain datasets and performs on-par with the baseline on \data{BioNLP13CG} demonstrating the effectiveness of our proposed division of labor. 
    
    \item The results show that span detection and classification tasks have minimal correlation with each other and can be done independent of each other. 
    
    \item Compared to the baseline approaches, \modelname{} has more representative power. This is because we have two BERT models (rather than one) each working on their own sub-tasks and contributing towards better NER. % while the baselines just have a single model.
    
    \item Comparison with baselines gives some additional insights into the internal functioning of BERT model which also implicitly tries to learn entity-agnostic extraction rules for mentions. Our approach explicitly models this behavior and hence is found to be more effective.
    
    \item \data{WNUT17} has a diverse range of rare and emerging entities crudely categorized into $6$ entity types. A single-model NER system may get confused and try to learn sub-optimal entity-specific extraction rules. Our task segregation allows \spandetect{} to form generalized extraction rules which is found to be more effective as shown in Table \ref{tab:main}.
    
    \item As a sidenote, all our baselines and \modelname{} outperform the previously published approaches on \data{BioNLP13CG} (Table \ref{tab:main}), thus setting new state-of-the-art results. However, the credit here goes to the SciBERT model, 
    better training parameters and the added character and pattern features.
    
    \item \modelname{} leverages the QA framework much more efficiently than the standard single-model NER systems (Table \ref{tab:train_time_ablation}). The margin of improvement is more pronounced with the increase in data size and number of entity types.
    
    \item Table \ref{tab:train_time_ablation} considers the components of \modelname{} being trained sequentially. However, the approach is flexible enough to train both models in parallel, reducing the train time even further. The sequential nature is necessary only at inference time.
    
    % \item Comparing the F1 scores in Table \ref{tab:det_ablation} and Table \ref{tab:class_ablation}, detecting correct mention spans is harder than disambiguating them and classifying them into entity types. However, both the sub-tasks are simpler than the overall NER task.
    
    \item From results in Table \ref{tab:det_ablation}, adding character and pattern features indeed helps detect better boundaries improving precision while maintaining recall thus giving better F1 score in \spandetect{}.
    
    \item Further, Table \ref{tab:feature_ablation} shows that adding character and pattern features independently helps span detection. The effect is more pronounced when both are added together. However, adding POS features does not help, indicating that POS semantics is well captured by existing BERT features and the added character and pattern features.
    
    \item Table \ref{tab:class_ablation} shows that dice loss helps handle the class imbalance issues seen in \spanclass{} and gives a slight performance improvement over cross entropy loss.
\end{itemize}

\subsection{Qualitative Analysis}
Table \ref{tab:quality} shows some sample predictions by \modelname{} and compares them  with \method{BERT-QA}. Note that \method{BERT-QA} is a standard single-model NER system and does not use our proposed character and pattern features or dice loss. We observe that,
\begin{itemize}
    \item \modelname{} is better in detecting emerging entities and out-of-vocabulary (OOV) terms (for example, movie titles and softwares) which may be rare and have high diversity. This can be attributed to \spandetect{} being stronger in generalizing and sharing entity extraction rules across multiple entity types.
    
    \item \method{BERT-QA} gets confused when entities have special symbols within them (for example, hyphens and commas). Our character and pattern features in \spandetect{} help handle such cases well.
    
    \item \method{BERT-QA} model develops a bias towards more common entity types like \type{Location} and misclassifies rare entity mentions (like \type{Product}) when they occur in a similar context. \modelname{} handles such cases well with the help of a dedicated \spanclass{} using dice loss.
\end{itemize}

\begin{table}
\centering
\begin{small}
\begin{tabular}{cl}\toprule
 Category &  {~~~~~~~~~~~~}Example Sentence \\\toprule
\multirow{2}{*}{\shortstack{General \\ Detection}}     
     & CVS selling their own version of ... \\
     & \textit{\textcolor{red}{CVS}} selling their own version of ... \\ \midrule
\multirow{2}{*}{\shortstack{Emerging \\ Entities}}  
    &Rogue One create a plot hole in Return of the Jedi ...  \\
     & \textit{\textcolor{Turquoise}{Rogue One}} create a plot hole in \textit{\textcolor{Turquoise}{Return of the Jedi}} ...  \\ \midrule
\multirow{2}{*}{\shortstack{Scientific \\ Terms}} 
     & Treating EU - 6 with anti-survivin antisense ...  \\
     & Treating \textit{\textcolor{brown}{EU - 6}} with anti-survivin antisense ... \\ \midrule
\multirow{2}{*}{Boundary}  
   & .. Hotel Housekeepers Needed in Spring , \textit{\textcolor{ForestGreen}{TX}} ...  \\
    & .. Hotel Housekeepers Needed in \textit{\textcolor{ForestGreen}{Spring , TX}} ...  \\ \midrule
\multirow{2}{*}{\shortstack{OOV \\ Terms}}  
   & Store SQL database credentials in a webserver \\
    & Store \textit{\textcolor{magenta}{SQL}} database credentials in a webserver \\ \midrule
 \multirow{2}{*}{\shortstack{Entity \\ Type}}  
  & Why do so many kids in \textit{\textcolor{ForestGreen}{Digimon}} wear gloves? \\
    & Why do so many kids in \textit{\textcolor{magenta}{Digimon}} wear gloves? \\ \bottomrule
\end{tabular}
\caption{Qualitative comparison of \modelname{} and \method{BERT-QA} systems. For each category, the first line shows the result of \method{BERT-QA},
and the second line shows the result of \modelname{}. 
% The words in italics are the entity mentions extracted by the systems, and red, blue, brown, green, and magenta colors denote \type{Org}, \type{Creative Work}, \type{Gene},  \type{Location} and \type{Product}  types respectively.
The words in italics are the entity mentions extracted by the systems color-coded as \textcolor{red}{\type{Org}}, \textcolor{Turquoise}{\type{Creative Work}}, \textcolor{brown}{\type{Gene}}, \textcolor{ForestGreen}{\type{Location}} and \textcolor{magenta}{\type{Product}}
}
    \label{tab:quality}
\end{small}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% old version
\if false
\begin{table*}[h!]
\centering
\begin{small}
\begin{tabular}{ccc}\toprule
Category & Model & Example \\\toprule
\multirow{2}{*}{General Detection} & \bertqa{} & \textit{CVS selling their own version of ...} \\
    & \modelname{} & \textit{\textcolor{blue}{CVS}[\texttt{Corporation}] selling their own version of ...} \\ \midrule
\multirow{2}{*}{Emerging Entities} & \bertqa{} & \textit{Does Rogue One create a plot hole in Return of the Jedi ... } \\
    & \modelname{} & \textit{Does \textcolor{blue}{Rogue One}[\texttt{Creative Work}] create a plot hole in \textcolor{blue}{Return of the Jedi}[\texttt{Creative Work}] ... } \\ \midrule
\multirow{2}{*}{Scientific Terms} & \bertqa{} & \textit{The MVD and Ki - 67 LI were higher ... } \\
    & \modelname{} & \textit{The MVD and \textcolor{blue}{Ki - 67}[\texttt{Gene}] LI were higher ...} \\ \midrule
\multirow{2}{*}{Boundaries} & \bertqa{} & \textit{.. Hotel Housekeepers Needed in Spring , \textcolor{blue}{TX}[\texttt{Location}] ... } \\
    & \modelname{} & \textit{.. Hotel Housekeepers Needed in \textcolor{blue}{Spring , TX}[\texttt{Location}] ... } \\ \midrule
\multirow{2}{*}{Out of Vocab Terms} & \bertqa{} & \textit{Store SQL database credentials in a webserver} \\
    & \modelname{} & \textit{Store \textcolor{blue}{SQL}[\texttt{Product}] database credentials in a webserver} \\ \midrule
\multirow{2}{*}{Entity Classification} & \bertqa{} & \textit{Why do so many kids in \textcolor{blue}{Digimon}[\texttt{Location}] wear gloves?} \\
    & \modelname{} & \textit{Why do so many kids in \textcolor{blue}{Digimon}[\texttt{Product}] wear gloves?} \\ \bottomrule
\end{tabular}
\caption{Examples from multiple datasets comparing performance of \modelname{} and \bertqa{} systems}
    \label{tab:quality}
\end{small}
\end{table*}


\begin{table}[h!]
\centering
\begin{small}
\begin{tabular}{p{0.5in}p{0.5in}p{2in}}\toprule
 Category & Model & Example \\\toprule
\multirow{2}{*}{\shortstack{General \\ Detection}} &     
    \method{BERT-QA} & \textit{CVS selling their own version of ...} \\
    & \modelname{} & \textit{\textcolor{blue}{CVS}[\type{ORG}] selling their own version...} \\ \midrule
\multirow{2}{*}{\shortstack{Emerging \\ Entities}} & 
  \method{BERT-QA} & \textit{Does Rogue One create a plot hole in Return of the Jedi ... } \\
     &  \modelname{} & \textit{Does \textcolor{blue}{Rogue One}[\type{Creative Work}] create a plot hole in \textcolor{blue}{Return of the Jedi}[\type{Creative Work}] ... } \\ \midrule
\multirow{2}{*}{\shortstack{Scientific \\ Terms}} & 
    \method{BERT-QA} & \textit{The MVD and Ki - 67 LI were higher ... } \\
     & \modelname{} & \textit{The MVD and \textcolor{blue}{Ki - 67}[\type{Gene}] LI were higher ...} \\ \midrule
\multirow{2}{*}{Boundary} & 
  \method{BERT-QA} & \textit{.. Hotel Housekeepers Needed in Spring , \textcolor{blue}{TX}[\type{Location}] ... } \\
    & \modelname{} & \textit{.. Hotel Housekeepers Needed in \textcolor{blue}{Spring , TX}[\type{Location}] ... } \\ \midrule
\multirow{2}{*}{\shortstack{OOV \\ Terms}} & 
   \method{BERT-QA} & \textit{Store SQL database credentials in a webserver} \\
    & \modelname{} & \textit{Store \textcolor{blue}{SQL}[\type{Product}] database credentials in a webserver} \\ \midrule
 \multirow{2}{*}{\shortstack{Entity \\ Type}} & 
  \method{BERT-QA} & \textit{Why do so many kids in \textcolor{blue}{Digimon}[\type{Location}] wear gloves?} \\
    &  \modelname{} & \textit{Why do so many kids in \textcolor{blue}{Digimon}[\type{Product}] wear gloves?} \\ \bottomrule
\end{tabular}
\caption{Examples from multiple datasets comparing performance of the \modelname{} and \bertqa{} systems. \method{BERT-QA} refers \bertqa{} in this table.}
    \label{tab:quality}
\end{small}
\end{table}

\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% old version