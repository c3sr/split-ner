\subsection{Data}
 
We validate our system using three publicly available datasets  belonging to general (OntoNotes and WNUT) and biomedical (BioNLP13CG) domains
 and a private dataset from the cybersecurity domain. 
 The cybersecurity domain data contains news articles, blogs and technical reports related to malware and vulnerabilities, and
 we denote the dataset as \texttt{CyberThreats} in this paper.
Specifically, we demonstrate the effectiveness of our algorithms for non-word, low-resource entity recognition using the datasets from multiple domains and noisy text.
The datasets include not only traditional entity types with word mentions(e.g., PERSON, LOCATION) but also many entity types with non-word, very long mentions.  
Table \ref{tab:datasets_summary} shows a summary of the datasets.
\begin{table*}[h!]
\centering
\begin{small}
\begin{tabular}{ccccrrr}\toprule
 \textbf{Dataset} & \texttt{\#Entities} & \texttt{Non-Word} & \texttt{Low Resource} & \texttt{Train} & \texttt{Dev.} & \texttt{Test} \\ \toprule 
BioNLP13CG & 16 & Yes & Yes & 3,033  & 1,003 & 1,906 \\
%JNLPBA     & 5 & Yes & No & 16,807 & 1,739 & 3,856 \\
%CoNLL2003 & 4 & No & No & 14,041 & 3,250 & 3,453 \\
CyberThreats & 8 & Yes & Yes & 38,721 & 6,322 & 9,837 \\
OntoNotes5.0 & 18 & Yes & Yes & 115,812 & 15,680 & 12,217 \\  
WNUT17 & 6 & Yes & Yes & 3,394 & 1,287 & 1,009\\
\bottomrule
\end{tabular}
\caption{Overview of the experimental datasets. \texttt{\#Entities} indicates the number of unique entity types.
\texttt{Non-Word} and \texttt{Low Resource} indicate if the dataset contains non-word mentions (e.g, mentions with digits and symbols) and
low-resource entity types respectively.  
\texttt{Train}, \texttt{Dev.} and \texttt{Test} show the number of sentences in the datasets.}
\label{tab:datasets_summary}
\end{small}
\end{table*}

\yjcomment{Shall we show the class distribution for the datasets? We need to use the names of diffrent methods consistently. Let's make a name for our system instead of Span-Pipeline.}
\comment{Different domains (entity nature \& language context) / dataset sizes / No. of entities 
- BIONLP13CG: Bio dataset (using Scibert + BERT model)
- OntoNotes: General entities / newswire (using Roberta base)
- WNUT17: emerging entities from tweets (using BERT)
Also show from result analysis that span class is simpler and gives above 90\% result where as detector is the main bottleneck}

\begin{table*}[h!]
\centering
\begin{small}
\begin{tabular}{ccccc}\toprule
 \textbf{Model} & \texttt{BioNLP13CG} & \texttt{CyberThreats} & \texttt{OntoNotes} & \texttt{WNUT17} \\ \toprule 
BERT-QA & \textcolor{red}{todo} & \textcolor{red}{todo} & \textcolor{blue}{ongoing(y4)}  & \textcolor{blue}{ongoing(y1)} \\
BERT-Span-Pipeline     & 86.70 & \textcolor{blue}{ongoing(y1)} & 90.31 & 56.30  \\
Reported SOTA & 85.56(thesis see) & N/A & 92.07(MRC-Dice) & 60.45(CL-KL)  \\
\bottomrule
\end{tabular}
\caption{Results. \texttt{\#F1} mention-level Micro F1\%. \yjcomment{let's compare with SOTA without any external data or additinonal pretraining. I consider MRC-Dice also using additional info. as they added several synonyms/hyponyms in query.   we can put SOTA without external data and with external data in separate rows if you like. We point out that our system does not rely on external knowledge which is usually unavailable for domain-speicific data and needs less computing resources than MRC}}
\label{tab:main}
\end{small}
\end{table*}

\if false
\begin{table*}[h!]
\centering
\begin{small}
\begin{tabular}{cccc}\toprule
 \textbf{Model} & \texttt{BioNLP13CG} & \texttt{OntoNotes} & \texttt{WNUT17} \\ \toprule 
BERT-Span-Pipeline     & 78.14 & 86.27(best) & 50.15  \\
Reported SOTA & N/A & 86.9(few-shot-paper-unpublished) & 50.5(CL-KL)  \\
\bottomrule
\end{tabular}
\caption{Results. (10\% setting) (may remove this table if not very important) \texttt{\#F1} mention-level Micro F1\%.}
\label{tab:main}
\end{small}
\end{table*}
\fi

\begin{table*}[h!]
\centering
\begin{small}
\begin{tabular}{ccccc}\toprule
 \textbf{Model} & \texttt{BioNLP13CG} & \texttt{CyberThreats} & \texttt{OntoNotes} & \texttt{WNUT17} \\ \toprule 
Span-Detector-CharPattern & 91.06 & 78.63 & 92.50 & 55.21  \\
Span-Detector-Vanilla     & 90.67 & \textcolor{blue}{scheduled(y1)} & \textcolor{red}{todo} & 54.85  \\
\bottomrule
\end{tabular}
\caption{Span Detector Results. \texttt{\#F1} mention-level Micro F1\%.}
\label{tab:det_ablation}
\end{small}
\end{table*}

\begin{table*}[h!]
\centering
\begin{small}
\begin{tabular}{ccccc}\toprule
 \textbf{Model} & \texttt{BioNLP13CG} & \texttt{CyberThreats} & \texttt{OntoNotes} & \texttt{WNUT17} \\ \toprule 
Span-Class-Dice & 94.27 & \textcolor{blue}{ongoing(y1)} &  96.74 & 73.40  \\
Span-Class-CE     & 94.04 & 87.58 & \textcolor{red}{todo} & 73.31  \\
\bottomrule
\end{tabular}
\caption{Span Classifier Results. \texttt{\#F1} mention-level Micro F1\%.}
\label{tab:class_ablation}
\end{small}
\end{table*}

\begin{table*}[h!]
\centering
\begin{small}
\begin{tabular}{ccccc}\toprule
 \textbf{Model} & \texttt{BioNLP13CG} & \texttt{CyberThreats(10\%)} & \texttt{OntoNotes(10\%)} & \texttt{WNUT17} \\ \toprule 
QA                & 1,372.8 & 877.1 &  7,381.8   & 568.2\\
Span-Pipeline     & 241.2 (106.3/134.9) & 145.53 (113.7/31.9) & 300.8 (185.5/115.3)  & 122.9 (98.9/24.0)\\
\bottomrule
\end{tabular}
\caption{Comparison of the training time between our method and a non-pipelined QA-based NER method. 
    The training time is reported in seconds per training epoch. The numbers in the parentheses denote the training times for span detection and span classification. }
\label{tab:train_time_ablation}
\end{small}
\end{table*}

\subsection{Experimental Setup}
We report all our results on the test sets after taking the model checkpoint corresponding to the best micro-averaged F1-score on development set. The development set evaluation takes place at steps of 0.5 training epochs. We train the models for $300$ epochs at learning rate $10^{-5}$ unless otherwise specified.

We use \texttt{transformers}\footnote{https://github.com/huggingface/transformers} python library by HuggingFace and \texttt{pytorch} for implementation and fix random seed to $42$ for replication. For general English corpora like \texttt{CoNLL 2003} and \texttt{OntoNotes 5.0}, by default, we use the pretrained \texttt{bert-base-uncased}\footnote{https://huggingface.co/bert-base-uncased} model. For biomedical datasets, \texttt{BioNLP13CG} and \texttt{JNLPBA}, we use \texttt{BioBERT-Base}\footnote{https://github.com/dmis-lab/biobert\#download} model. Note that in all our experiments, we only use the BERT-Base architecture which has around 110M trainable parameters. We use \texttt{Nvidia GeForce GTX 1080} and \texttt{Nvidia Tesla V100} gpus for model training and evaluation.

We use cross entropy loss during training unless otherwise specified. The training data is randomly shuffled and a batch size of $16$ is used with post-padding. For BERT-based models, we fix maximum sequence length to $256$ for \texttt{BioNLP13CG}, \texttt{CoNLL 2003}, \texttt{JNLPBA} datasets and $512$ for \texttt{OntoNotes 5.0} data. Unless otherwise specified, the BERT-based models output entity labels for each sub-token (as per WordPiece tokenization) of an existing token in the dataset. As a heuristic, we take the label of first sub-token as the label for the corresponding token during our evaluations.

\subsection{Baseline Systems}
\comment{Insted of BERT-xxx, people usually use the system name if it is well known (e.g., MRC) or the paper citation to denote the systems.}


\texttt{BERT}: Proposed by \cite{devlin2018bert}, BERT is a bidirectional encoder transformer\cite{vaswani2017attention}. It applies WordPiece\cite{wu2016google} tokenization on input sentence which is then passed through several encoder layers with multiple attention heads capturing sentence semantics and inter-token relationships well. The model outputs contextualized embeddings for each sub-token in the sentence. We take the last hidden layer outputs from BERT model and pass it to a fully connected layer. The outputs are converted to a probability distribution over labels space. Model parameters are initialized from a pretrained model and fine-tuned on our NER task.


\texttt{BERT-Freeze}: To understand how much semantic information is already captured in a pretrained BERT model, we use the exact same architecture as \texttt{BERT} model above but freeze the BERT model parameters. So, the only trainable parameters remain from the fully connected layer. For this setting, we use learning rate as $0.005$.

\texttt{CNN-LSTM-CRF}: \comment{better to include this as a baseline since it is very widely used}


\subsection{Performance Comparison}
Table \ref{tab:res_span} reports the results of the pipelined span detection and classification procedure and compares it with simple BERT QA setup. We present this comparison since QA model serves as the primary backbone of our span-based approach. All models here use \texttt{BIOE} tagging scheme and use \textit{What} as the question word in question formulation.

\begin{table*}[h!]
\centering
\begin{small}
\begin{tabular}{cccccc}\toprule
      &  \textbf{} & BioNLP13CG & JNLPBA & CoNLL2003 & OntoNotes5.0\\\toprule
\multirow{3}{*}{Baseline} & \texttt{BERT-Freeze} &  75.42 & 55.93 & 82.79 & 67.35 \\
                          & \texttt{BERT} & 85.99 & 74.35 & 91.36 & 83.39 \\ 
                          & \texttt{BERT-QA} & \textbf{86.45} & 74.81 & 91.17 & \\\midrule
\multirow{3}{*}{OurModel} &        \texttt{Span Detection} & 90.12 & 78.35 & 95.23 & \\
        & \texttt{Span Classification} & 94.06 & 95.08 & 94.50 & \\
        & \texttt{Pipeline} & 85.89 & \textbf{75.01} & \textbf{91.64} & \\ \bottomrule
\end{tabular}
\caption{The classificaiton results of our system and the state of the art method over 4 benchmark datasets. 
     The numbers reprent the Micro-F1 in \% on the test datasets.}
    \label{tab:res_span}
\end{small}
\end{table*}

\comment{need more results and some plots}

\if false
Next, we deep dive into the \texttt{BioNLP13CG} dataset which has $16$ entity types including several high and low-resource types. We compare the model performance at the entity type level for our 3 major NER approaches: sequence labeling, question answering and span-based pipeline. We compare our best performing model variants through entity-level and macro-averaged F1-scores. Let $\mathcal{T}$ be the set of all entity types and F1$_t$ be the F1-score for individual entity type $t \in \mathcal{T}$. Then, Macro-averaged F1-Score is defined in Equation \ref{eq:macro_f1} as:
\begin{equation}
\label{eq:macro_f1}
    \text{Macro-F1} = \frac{1}{\mathcal{\vert\mathcal{T}\vert}}\,\sum_{t\,\in\,\mathcal{T}}{\text{F1}_t}
\end{equation}
We present the comparison among the following models:
\begin{itemize}
    \item \texttt{Dice Loss}: Sequence labeling NER approach over BERT model with \texttt{BIO} tagging scheme and dice loss instead of cross entropy.

    \item \texttt{Special Symbol}: Sequence labeling NER approach over BERT with \texttt{BIO} tagging scheme and additional one-hot input feature to capture if a token is a special symbol like \textit{hyphen}, or \textit{parenthesis}.

    \item \texttt{BERT-QA (Where)}: Question answering NER approach with \texttt{BIOE} tagging scheme and \textit{Where} as the question word.

    \item \texttt{Span Based}: Pipelined approach which uses the QA setup with \texttt{BIOE} tagging scheme and \textit{What} as question word for span detection and QA-based sequence classification for span classification.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{../thesis/high_resource_entity_metrics}
    \caption{Test set Entity-level F1 scores for high resource entities in \texttt{BioNLP13CG} dataset}
    \label{fig:high_resource_entity_metrics}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{../thesis/low_resource_entity_metrics}
    \caption{Test set Entity-level F1 scores for low resource entities in \texttt{BioNLP13CG} dataset}
    \label{fig:low_resource_entity_metrics}
\end{figure}

\fi
