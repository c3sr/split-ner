Before diving into our proposed framework, we start by re-implementing a standard single-model QA-based NER system over the BERT-base architecture. In this setup, the model is fed a query specifying the entity to extract followed by the input sentence. For example, \textit{``Extract \texttt{person} from the following text. Emily lives in United States"}. The model is trained to understand the entity semantics from the query and output the corresponding mention spans from the sentence. Each token in the input is classified into one of the four output classes: \texttt{B} (begin), \texttt{I} (intermediate), \texttt{E} (end) and \texttt{O} (none). Spans are created from this output following \texttt{BIOE} tagging scheme. We call this as the \method{BERT-QA} model and train it on four different corpora (Table \ref{tab:datasets_summary}). 
Looking at the model predictions (both correct and incorrect) we find that,
\begin{enumerate}
    \item Mentions of same entity type can occur in diverse linguistic contexts.
    \item Mentions belonging to different entity types can occur in very similar linguistic contexts.
\end{enumerate}

Table \ref{tab:findings} gives some examples. These findings reveal that we don't need entity-specific extraction rules and that the task of extracting a mention span can be decoupled from the task of classifying it into an entity type. 
\begin{table}[htbp]
\centering
\begin{small}
\begin{tabular}{l}\toprule
 Examples of same entity appearing in different contexts \\\midrule
%  \textit{Investors were confident after watching success of \textcolor{blue}{Musk} in \textcolor{red}{Tesla}} \\
    \textit{People say \textcolor{blue}{Musk} and \textcolor{red}{Tesla} have a lot in common ...} \\
    \textit{With \textcolor{blue}{Musk}, \textcolor{red}{Tesla} is bound to have a promising future.} \\
    \textit{\textcolor{blue}{Musk} announced \textcolor{red}{Tesla's} new model ...} \\
     \textit{\textcolor{blue}{Musk's} \textcolor{red}{Tesla} has features not seen elsewhere.} \\
    \textit{The discussion was moderated by \textcolor{red}{Tesla's} \textcolor{blue}{Elon Musk}.} \\ \midrule \midrule
Examples of different entities in similar contexts \\ \midrule
\textit{Why do so many kids in \textcolor{magenta}{Digimon} wear gloves?} \\
\textit{Why do so many people in \textcolor{ForestGreen}{Switzerland} speak German?} \\
 \textit{\textcolor{red}{Tesla} %[\texttt{Org}] 
 is bound to have a promising future with \textcolor{blue}{Musk}.} \\ %[\texttt{Person}].} \\
 \textit{\textcolor{blue}{Musk} %[\texttt{Person}] 
 is bound to have a promising future with \textcolor{red}{Tesla}.} %[\texttt{Org}].} 
 \\ \bottomrule
\end{tabular}
\caption{Examples showing that mention span detection and entity classification can be treated as independent tasks. Entity types are color-coded as \textcolor{blue}{\type{Person}}, \textcolor{red}{\type{Org}}, \textcolor{magenta}{\type{Product}} and \textcolor{ForestGreen}{\type{Location}}.}
    \label{tab:findings}
\end{small}
\end{table}

\if false
\begin{table}[htbp]
\centering
\begin{small}
\begin{tabular}{cl}\toprule
Category & Example \\\toprule
% \multirow{2}{*}{Same entity in different contexts} & \textit{.. a company created by her mother, Rosie ...} \\
%     & \textit{.. a company which her mother, Rosie created ...} \\
%     & \textit{.. Rosie, her mother, created a company ...} \\ \midrule
\multirow{4}{*}{\shortstack{Same \\ entity in \\ different  \\ contexts}} & \textit{Investors were confident after watching success of \textcolor{blue}{Musk} in \textcolor{red}{Tesla}} \\
    & \textit{When \textcolor{blue}{Musk} announced \textcolor{red}{Tesla's} new model, their share price ...} \\
    & \textit{People say \textcolor{blue}{Musk} and \textcolor{red}{Tesla} have a lot in common ..} \\
    & \textit{With \textcolor{blue}{Musk}, \textcolor{red}{Tesla} is bound to have a promising future.} \\
    & \textit{\textcolor{blue}{Musk's} \textcolor{red}{Tesla} has features not seen elsewhere.} \\
    & \textit{The conference was spearheaded by \textcolor{red}{Tesla's} \textcolor{blue}{Elon Musk}.} \\\midrule
\multirow{4}{*}{\shortstack{Different \\ entities in \\ similar \\ contexts}} & \textit{Why do so many kids in \textcolor{blue}{Digimon}[\texttt{Product}] wear gloves?} \\
    & \textit{Why do so many people in \textcolor{blue}{Switzerland}[\texttt{Location}] speak German?} \\
    & \textit{\textcolor{red}{Tesla}[\texttt{Org}] is bound to have a promising future with \textcolor{blue}{Musk}[\texttt{Person}].} \\
    & \textit{\textcolor{blue}{Musk}[\texttt{Person}] is bound to have a promising future with \textcolor{red}{Tesla}[\texttt{Org}].} \\ \bottomrule
    % \multirow{2}{*}{Different entities in similar contexts} & \textit{With the help of Lemon Cheer[\texttt{Product}], P\&G's[\texttt{Org}] share increases ...} \\
    % & \textit{With the help of Yahoo[\texttt{Org}], Verizon's[\texttt{Org}] share increases ...} \\ \bottomrule
\end{tabular}
\caption{Examples showing that mention span detection and entity classification can be treated as independent tasks}
    \label{tab:findings}
\end{small}
\end{table}
\fi

Hence, we propose a two-stage pipelined system, \modelname{},  which has independent dedicated modules to focus on mention span detection and entity classification separately. Together, the pipeline solves the NER task. Further, this gives us the flexibility to design models tailor-made to cater to the needs of the individual module-specific sub-tasks.
