In the past few years, deep learning approaches have been increasingly applied for NER \cite{torfi2020natural, li2020survey}, a popular architecture being CNN-LSTM-CRF \cite{ma2016end}. 
% Training complex deep learning systems have however been limited by the shortage of labeled training data. Nevertheless, there have been efforts to generate noisy labeled data as weak supervision signals for training. \cite{shang2018learning} propose AutoNER framework which uses distant supervision, \cite{arora2017extracting} use regular expression patterns for artificial training data generation, \cite{zhou2019dual} propose adversarial perturbations and \cite{liu2018empower, liu2018efficient} train a task-aware language model from unlabeled data which guides NER. 
With the recent success of BERT \cite{devlin2019bert}, it has become possible to transfer knowledge from massive pretrained language models by fine-tuning them on small datasets to get good performance.

\cite{xu2021better} propose a Syn-LSTM setup leveraging dependency tree structure with pretrained BERT embeddings for NER. \cite{li2020MRC} and \cite{li2019dice} %have a system very similar to our system and 
use a QA-based setup over BERT for NER similar to our system.  
Recently \cite{yan2021unified} propose a generative framework leveraging BART \cite{lewis2019bart} for NER. \cite{yu2020named} propose a biaffine model utilizing pretrained BERT and FastText \cite{bojanowski2017enriching} embeddings along with character-level CNN setup over a Bi-LSTM architecture. All of these models leverage BERT and report good performance on \data{OntoNotes5.0}, however, using BERT-Large architecture. 

\cite{wang2021improving} give good performance on \data{WNUT17} by leveraging external knowledge and a cooperative learning setup. Recently \cite{nguyen2020bertweet} propose a new BERT model, BERTweet, trained on English tweets. Currently, we use the standard pretrained BERT model by \cite{devlin2019bert} on \data{WNUT17}. Our framework can easily be trained using BERTweet and this can enhance our results on \data{WNUT17} even further.

In scientific and biomedical domain, NER has its own set of challenges. Entity mentions are long and may have alpha-numeric symbols and chemical formulas which is hard for a language model to capture well. 
On \data{BioNLP13CG}, \cite{crichton2017neural} report $78.90$ F1 in a multi-task learning setup and \cite{neumann2019scispacy} report $77.60$ using the SciSpacy system.
Note that \modelname{} outperforms both systems by a large margin.

\if false
BioBERT~\cite{lee2020biobert} authors does not state their performance on this dataset however \cite{banerjee2019knowledge} replicate the BioBERT model as a baseline and report $85.56$. Other works like \cite{wang2019cross} use a multi-task learning framework and combine labelled data from multiple corpora mapping entity tags across corpora to coherent classes. \cite{wang2019distantly} make use of a setup similar to AutoNER~\cite{shang2018learning} tailor-made for biomedical NER. 
\fi 

Finally, \cite{li2020MRC,Jiang20,Ouchi20} perform NER as a span prediction task. However, they enumerate all possible span start and end locations in a sentence leading to an $\mathcal{O}(n^2)$ complexity where $n$ denotes the sentence size. Our architecture design does a token level classification and hence has $\mathcal{O}(n)$ complexity.
