In the past few years, deep learning approaches have been increasingly applied for NER \cite{torfi2020natural, li2020survey}, a popular architecture being CNN-LSTM-CRF\cite{ma2016end}. 
% Training complex deep learning systems have however been limited by the shortage of labeled training data. Nevertheless, there have been efforts to generate noisy labeled data as weak supervision signals for training. \cite{shang2018learning} propose AutoNER framework which uses distant supervision, \cite{arora2017extracting} use regular expression patterns for artificial training data generation, \cite{zhou2019dual} propose adversarial perturbations and \cite{liu2018empower, liu2018efficient} train a task-aware language model from unlabeled data which guides NER. 
Recently with the advent of BERT\cite{devlin2019bert}, it has become possible to transfer knowledge from massive pretrained language models and fine-tune them giving good performance even on small datasets.

\cite{xu2021better} propose a Syn-LSTM setup leveraging dependency tree structure along with pretrained BERT embeddings for NER. \cite{li2020MRC} and \cite{li2019dice} have a system very similar to our system and use QA-based setup over BERT for NER. However, they enumerate all possible span start and end locations in a sentence leading to an $\mathcal{O}(n^2)$ complexity where $n$ denotes the sentence size. Our architecture design does a token level classification and hence has $\mathcal{O}(n)$ complexity. Recently, \cite{yan2021unified} propose a generative framework leveraging BART\cite{lewis2019bart} for NER. \cite{yu2020named} propose a biaffine model utilizing pretrained BERT and FastText \cite{bojanowski2017enriching} embeddings along with character-level CNN setup over a bi-LSTM architecture. All of these models leverage BERT in some way or the other and report good performance on \data{OntoNotes 5.0} corpus however all of them make use of BERT-Large architecture. 

\cite{wang2021improving} give good performance on \data{WNUT17} dataset by leveraging external knowledge and a cooperative learning setup. Recently, \cite{nguyen2020bertweet} propose a new BERT model, BERTweet trained on English tweets. Currently, we use the standard pretrained BERT model by \cite{devlin2019bert} on \data{WNUT17}. Our framework can easily be trained using BERTweet model as the backbone and this can enhance our results on \data{WNUT17} even further.

In scientific and biomedical domain, NER has its own set of challenges. Entity mentions are long and may have alpha-numeric symbols and chemical formulas which may be hard for a language model to make sense of. On \data{BioNLP13CG} corpus, \cite{crichton2017neural} report $78.90$ as test set micro-F1 in a multi-task learning setup and \cite{neumann2019scispacy} report $77.60$ using their SciSpacy system. BioBERT\cite{lee2020biobert} authors does not state their performance on this dataset however \cite{banerjee2019knowledge} replicate the BioBERT model as a baseline and report $85.56$. Other works like \cite{wang2019cross} use a multi-task learning framework and combine labelled data from multiple corpora mapping entity tags across corpora to coherent classes. \cite{wang2019distantly} make use of a setup similar to AutoNER\cite{shang2018learning} tailor-made for biomedical NER. 
