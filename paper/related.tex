\textbf{CrossWeigh}~\cite{WangSLLLH19} considers a method to handle label mistakes during NER model training.

It partitions the training data into several folds and train independent NER mod- els to identify potential mistakes in each fold and then adjusts the weights of training data accordingly to train the final NER model.
The technique is similar to k-fold cross validation with \textit{entity disjoint filtering}.
(1) split the training data into k folds.
(2) collect all entity mentions (only mentions and not entity types)  in a fold (i) (called test entities)
(3) remove all sentences that contain the test entities from the training data for the i-th fold 
(4) train k models and test them
(5) find sentences that contain at least one mis-classification
(6) compute confidence for each sentence indicating the likelihood the sentence containing  mislabels.
However, this method works well when the initial model has high confidence of being accurate (>90\% general accuracy).
·        Data Setting: CoNLL2003 English NER
·        Performance: Leads to ~0.5\% improvements due to label corrections

\textbf{Contextual string embeddings}~\cite{akbik-etal-2018-contextual} models words as sequences of characters contextualized by their surrounding text.
It passes  sentences as sequences of characters into a character-level language model to form word-level embeddings.
It leverages pre-trained character-level language models from which they extract hidden states at the beginning and end character positions of each word to produce embeddings for words.
The embedding of a word is the concatenation of the hidden states of fLM and bLM of its character sequences.
This results in different embeddings for the same word when it is used in different contexts and has shown a significant improvement for sequence labeling tasks such as NER and Chunking.
The main advantages are: (1) pre-train on large unlabeled corpora, (2) capture word meaning in context and therefore produce different embeddings for polysemous words depending on their usage, and (3) model words and context fundamentally as sequences of characters, to both better handle rare and misspelled words as well as model subword structures such as prefixes and endings.
For NER, the best performance was achieved by Contextual string embeddings + pretrained word embeddings + task-trained character features in a hierarchical BiLSTM-CRF architecture 
closely followed by Contextual string embeddings + pretrained word embeddings (Glove) indicating that task-trained character features are subsumed by character embeddings.


\textbf{W-NUT (Workshop on Noisy User-generated Text)} uses training data from Twitter and development/testing data from Reddit, YouTube, Twitter and StackOverflow data.
Entity types include Person, Location, Corporation, Product, Creative-work, Group.
The best F1 is currently 49.59 by Akbik et al.~\cite{akbik-etal-2019-pooled}.

\textbf{Pooled contextualized embeddings}~\cite{akbik-etal-2019-pooled} address the drawback of the contextualized character-level models which suffer 
from an inherent weakness when encountering ``rare words in an underspecified context``.
However, they assume that these rare entities are normally only used in underspecified contexts \textit{if they are expected to be known to the reader}. 
That is, they are either more clearly introduced in an earlier sentence, or part of general in-domain knowledge.

They aggregate the contextualized string embeddings of each unique string 
 and thus produces \textit{evolving word representations} that change over time as more instances of the same word appear. 

For each word in a sentence, the word representation is a concatenation of its contextualized word embedding generated by~\cite{akbik-etal-2018-contextual} and a pooling of
all embeddings of the word seen so far.

\textbf{A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers}~\cite{ChaudharyXSNC19}
proposes a bootstrapping approach for NER for low-resourced languages.
It applies a cross-lingual transfer learning to project annotations from English data to the target language (with word embeddings of both source and target languages, a bilingual dictionary and labeled data for the source language), then apply activie learning
and bootstrapping of the model.
To reduce human labeling efforts, they label only subspans of sentences where named entities most likely appear and apply partial CRFs to learn from the annotated subspans.
(see Algorithm 1 for the potential entity span detection method).

·        Idea: Transfer Learning then Bootstrapping (using manual active learning)
·        Data Setting: CoNLL English, Hindi, Spanish, Indonesian NER
·        Model: CNN-BiLSTM-CRF (for NER). Initially train Bilingual word embeddings then transfer knowledge to label unlabeled low-resource corpus. Train CNN-BiLSTM-CRF model for NER on it. Use marginal prob. to get entity spans with high uncertainty for human labeling. Augment to corpus and retrain NER model iteratively. Use partial-CRF for NER, since, sentences are partially gold annotated, for other parts of the sentence, the loss should not affect the model (since, those parts are not yet labelled)
·        Performance: +9 points in F1 (around) than other active learning or transfer learning techniques.

1. Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition~\cite{ZhouZJZFGK19}
·        Idea: adversarial modifications can help in knowledge transfer in low-resource settings
·        Data Setting: Cross-Lang (Eng to Spanish/Dutch), Cross-Domain (CoNLL English to Tweets English) knowledge transfer
·        Model: CNN-BiLSTM-CRF arch. with adversarial perturbations (introduced for robust training in low-resource setting)
·        GRAD loss (for shared parameters): basically linear transformation over self-attention then log likelihood of that.
o   Not sure how it would work without softmax? (1 - r) term: how does it work?
o   How is the linear transformation params decided?
·        Overall loss: label loss (source and target) (CRF) + GRAD loss
·        Performance: Comparable performance in high-resource and better performance in low-resource setting
 
 
 
4.      Improving Distantly-supervised Entity Typing with Compact Latent Space Clustering~\cite{PengXZFH19}
·        Idea: Distant supervision leads to noisy training data. Existing denoising methods rely on partial label loss objective which leads to confirmation bias (as it corrects data based on its own objective function). So, try to cluster similar entity mentions together instead.
·        Model: LSTM based for entity typing, minimizing KL divergence between predicted and actual entity labels
·        Compute embedding representation before passing to final classifier and make distance-graph out of them. Then, do label propagation on graph for clustering and revise embedding representation through a loss.
·        Data Setting: OntoNotes, BBN (news/wall street journal based)
·        Performance:  Compared to partial label loss, partial label embedding, joint representation of entity mentions and label types etc. approaches. 1-2\% F1 improvement.
 
5.      Exploiting Structure in Representation of Named Entities using Active Learning~\cite{BhutaniQ0JHV18}
·        Idea: Implicit structure in named entities (which is very task-dependent)
·        Model: Active Learning where user supplies some structures, model proposes more which the user can correct/reward. Edit distance among structures calculated. Rule generation based on pattern matches in corpus.
·        Data Setting: Person/Company mentions etc. from ACE
·        Performance: 3\% F1 better on company, similar on Person (than CRF-based)


Pattern (Regex) Understanding with Deep Learning:
1.      Deep Learning for Regex (Blog): Learning to identify alpha-numeric Product ID from text sentences (deep char-level embeddings learning implicit regular expression formats) (data/code not available)
 
2.      Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding (ACL'18): Matching regexes in sentences and using the matched words for manipulating attention weights etc. in a standard BiLSTM framework for sentence classification (intent detection) and sequence labelling (slot filing) tasks. Dataset: ATIS + manually created regexes (data available here)
 
3.      DeepRegex: Generating Regular Expression from Natural Language Description (EMNLP'16): Uses LSTM-based seq-2-seq arch., dataset(NX-RX, semi-manually created) has 10,000 regexes with single sentence descriptions. Code based on Pytorch and Lua (GitHub) (some positive and negative generated samples described here)
 
4.      Understanding Regex using Deep Learning (Blog): generating natural language description of regexes (opposite of DeepRegex). Used same data and model architecture. (data/code)
 
5.      Generating Regular Expressions from Natural Language Specifications: Are We There Yet? (AAAI'18): Basically, evaluates DeepRegex (described above) on NX-RX dataset and a real-world RegexLib dataset (not available). Says, NX-RX dataset is synthetic and in real-world, this system, DeepRegex, still does not perform too well.
a.      Proposes that large real-world corpus should be made for better training
b.      Model should use positive and negative examples of regex match, apart from the seq-2-seq arch. for better regex interpretation
 
6.      Inference of Regular Expressions for Text Extraction from Examples (TKDE'16): Automatically generating regular expressions from examples/given matches. (uses genetic algorithms) (slides, code, demo)
 
Numeral Understanding with Deep Learning:
1.      Numerical Polysemy (Challenge with numbers/alpha-numerics): Number '12' can have diverse meanings in different sentences based on context! It can be a month, date, age, size of plot etc. So, a single embedding for number 12 is not a good idea. Contextual embeddings would be better! Similar thing happens with words too, like, 'bank' but has less diverse range of meanings.
 
2.      Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments (ACL'19): Prepared data from market comments on Reuters. Predict number range (among 8 classes) for filling a blank related to the market comment. Found Bi-GRU framework working best. (Dataset)
 
3.      Do NLP models know numbers? (ACL'19): Pretty good analysis showing that Word2Vec, GloVe, char-CNN embeddings, ELMo, BERT all have some good sense of number representation. Tested on numerical Q/A dataset (DROP - made by AllenNLP). Found char-level embeddings are able to represent numbers better than word embeddings. CNNs arch. are able represent numbers better. ELMo gives better performance than BERT.
 
4.      Learning Numeral Embeddings(submitted to ICLR'20): Learning number embeddings by sampling some representative numbers out, embedding them, then for any other number, do soft clustering by learning a GMM over those representative number's embeddings.

