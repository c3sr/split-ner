Owing to its design, the \method{BERT-QA} system essentially creates an input sample for each possible entity type and sentence pair. This means for a dataset with \textit{N} sentences and $\mathcal{T}$ possible entity types, the overall dataset size becomes $\mathcal{T}$ $\times$ \textit{N}. This is a massive blow-up and as we see later, for large real-world datasets with many entity types (like \data{OntoNotes5.0}), \method{BERT-QA} can incur a heavy latency.

In such situations, our \modelname{} system comes to the rescue by training two light-weight models which train faster and together are more powerful than a standard single-model NER system. \modelname{} develops on top of a QA setup but removes all redundant inputs to the model. \spandetect{} just asks one question for each input sentence and identifies all required mentions together. For a dataset with $N$ sentences, our model's input is of size $N$. In \spanclass{}, we query once for each gold span during training or an identified span during inference. %This accounts for the bare minimum invocations to the model in a QA setup. 
Empirically, looking at \data{OntoNotes5.0} dataset, \method{BERT-QA} would fire $18$ queries (one for each entity type) for a given sentence. \modelname{} would query once in \spandetect{} and $1.36$ times in \spanclass{} (as per Table \ref{tab:datasets_summary}). This gives a significant latency improvement as can be seen later in Table \ref{tab:train_time_ablation}. 
