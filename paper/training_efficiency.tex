Owing to its design, the \texttt{BERT-QA} system essentially creates an input sample for each possible entity type and sentence pair. This means for a dataset with \textit{N} sentences and $\mathcal{T}$ possible entity types, the overall dataset size fed to the model becomes $\mathcal{T}$ $\times$ \textit{N}. This is a massive blow-up and as we see later, for large real-world datasets which span across many diverse entity types (like \data{OntoNotes 5.0}), the NER task can incur a heavy latency.

In such situations, our \texttt{2Q-NER} system comes to the rescue by training two light-weight models which train faster and together are more powerful than a standard single-model NER system. \texttt{2Q-NER} develops on top of a QA setup but removes all redundant inputs to the model. \textit{Span Detection Module} just asks one question for each input sentence and identifies all required mentions together. For a dataset with $N$ sentences, our model input is $N$. In \textit{Span Classification Module}, we query once for each gold span (during training) or identified mention span (during inference). This accounts for the bare minimum invocations to the model in a QA setup. Empirically, looking at \data{OntoNotes 5.0} dataset, a standard single-model QA system would fire $18$ queries (one for each entity type) for a given sentence. Our setup would query once in \textit{Span Detection Module} and $1.36$ times in \textit{Span Classification Module} as per Table \ref{tab:datasets_summary}. This gives a significant latency improvement as can be seen later in Table \ref{tab:train_time_ablation}. 
