In this work, we demonstrate that NER can be broken down into a pipeline of two simpler sub-tasks, span detection followed by span classification. Models can be trained for the sub-tasks independent of each other. Using the QA-framework over the BERT architecture, we show that this division of labor is not only effective but also significantly efficient, through experiments on multiple cross-domain datasets. Nevertheless, our approach has a pipeline-based structure and hence errors made during span detection propagate to the span classification stage. Sequence labeling and question answering approaches do not face this concern. However, our proposed breakdown does gives more flexibility to develop models and cater to the needs of the individual sub-tasks more effectively. We show that explicitly modeling character and pattern features helps detect better boundaries and dice loss helps handle class imbalance issues.

Through this paper, we open up the possibility of breaking down complex tasks into smaller sub-tasks and fine-tuning large pretrained language models for them individually. Interestingly, our span classification setup is the reverse of a standard QA-setup for NER. For a sentence, \textit{Emily lives in United States}, rather than asking a question of the form, \textit{``What is the \texttt{person} in the text?"} as done in NER, we ask \textit{``What is \texttt{Emily}?"}. This opens up prospects for more intuitive and creative forms of approaching NER and shows the power of BERT model yet again in capturing the semantics and query intent well. During our qualitative study, we identify that boundary detection serves as a primary bottleneck in NER performance and encourage the research community to design architectures or new training objectives tailor-made to handle mention boundaries more effectively. 
    
Currently, in our \textit{Span Detection Module}, all required entity mentions are being grouped into a single class. As a potential future work, we expect to get even better performance by a hierarchical extension of our setup, like a decision tree. At the top level, we detect mentions belonging to some crude categories and gradually break them down into more fine-grained buckets. These buckets can be designed to maximize information gain using some measure of entropy / similarity of mentions being grouped together.
