Named Entity Recognition (NER) is the task of extracting informing entities belonging to predefined semantic classes from raw text. As a common practice, existing systems model the NER problem as a whole.
% Existing NER methods perform mention detection and entity type classification in one model. 
However, in this paper we propose to break it down into two logical sub-tasks: (1) \textit{Span Detection} which extracts mention spans from a sentence irrespective of entity type; and (2) \textit{Span Classification} which classifies them into predefined semantic classes, thus completing the NER task. We formulate both our sub-tasks as question-answering (QA) problems and build over the BERT architecture.

Through experiments over four cross-domain datasets, we demonstrate that this logical separation is not only effective but also time efficient. Our  framework, \modelname{}, outperforms strong baselines on \data{OntoNotes5.0}, \data{WNUT17} and a cybersecurity dataset and gives on-par performance on \data{BioNLP13CG} corpus. In all cases, we see a significant reduction in training time.
The effectiveness of our system stems from fine-tuning the BERT model twice, separately for each sub-task, producing two leaner models which can be trained independently instead of a single large complex model.
% Further, this separation produces two leaner models instead of one large complex model and allows faster training since the two models can be trained in parallel.
% \comment{Add performance/time figures} 
% (1) how generalizable is this strategy
% (2) BERT model (black box deep model) seems to implicitly decouple the NER task this way

% \comment{Note: Readers' may say that any improvements you are getting could be due to training noise.. because we haven't conducted multiple experiments and not reported mean and std. deviation.. our main theme should be that, we are not trying to compete with vanilla NER in terms of performance! We are saying we are on-par with the vanilla NER performance and at the same time, getting immense improvements in time training efficiency!\\~\\ + flexibility that we give now that we have to focus on simpler sub-tasks..\\~\\The fact that our system is achieving similar results to normal NER also gives readers some insights into the BERT model and how it is handling NER as separate tasks internally!}