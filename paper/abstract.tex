Named Entity Recognition (NER) is the task of extracting informing entities belonging to predefined semantic classes from raw text. In this paper, we propose to break down the NER task into two logical sub-tasks: (1) \textit{Span Detection}, which extracts the spans of informing entities irrespective of their type and (2) \textit{Span Classification}, which simply classifies the extracted entities into predefined semantic classes. 
\yjcomment{add a sentence providing more details of the architecture. e.g. `the QA framework over the BERT..' below}
Both the sub-tasks leverage the full sentence structure and hence do not compromise on any essential information\yjcomment{not sure what this means. we may not need this in abstract}. 
We demonstrate that this logical separation and the QA framework are (1) effective and (2) time efficient through comprehensive experiments on multiple cross-domain datasets using a question-answering framework over the BERT architecture. \yjcomment{add a performance numbers later.}
The effectiveness stems by leveraging the power of the same language model twice, once for each sub-task. This type of modeling allows easy probing to identify performance bottlenecks at sub-task level and deploy personalized models to cater to them.
% (1) how generalizable is this strategy
% (2) BERT model (black box deep model) seems to implicitly decouple the NER task this way