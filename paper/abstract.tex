Named Entity Recognition (NER) is the task of extracting informing entities belonging to predefined semantic classes from raw text. In this paper, we propose to break down the NER task into two logical sub-tasks: (1) \textit{Span Detection}, which extracts the spans of informing entities irrespective of their type; and (2) \textit{Span Classification}, which simply classifies the extracted entity spans into the predefined semantic classes. 
We apply the question-answering (QA) framework over the BERT architecture for both the sub-tasks and leverage the full sentence structure and hence do not compromise on any essential information.

We demonstrate that this logical separation and the QA framework are  effective and  time efficient through comprehensive experiments on multiple cross-domain data sets. The effectiveness stems from leveraging the power of the same language model twice, once for each sub-task and the ability to optimize each model independently at the sub-task level.   
Further, this separation produces two leaner models instead of one large complex model and allows faster training since the two models can be trained in parallel.
% (1) how generalizable is this strategy
% (2) BERT model (black box deep model) seems to implicitly decouple the NER task this way