Named Entity Recognition (NER) is the task of extracting informing entities belonging to predefined semantic classes from raw text. In this paper, we propose to break down the NER task into two logical sub-tasks: (1) Span Detection, which extracts informing entities irrespective of their type and (2) Span Classification, which simply classifies the extracted entities into predefined semantic classes. Both the sub-tasks leverage the full sentence structure and hence do not compromise on any essential information. We demonstrate that this logical separation is (1) effective and (2) time efficient through our experiments on multiple cross-domain datasets using a question-answering framework over the BERT architecture. The effectiveness stems by leveraging the power of the same language model twice, once for each sub-task. This type of modeling allows easy probing to identify performance bottlenecks at sub-task level and deploy personalized models to cater to them.
% (1) how generalizable is this strategy
% (2) BERT model (black box deep model) seems to implicitly decouple the NER task this way