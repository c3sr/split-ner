NER has been a popular research topic in the field of natural language processing for long. The progress can be categorized into four classes namely, rule-based/dictionary-based techniques\cite{quimbaya2016named}, unsupervised methods\cite{zhang2013unsupervised}, feature-engineering approaches\cite{mcnamee2002entity} and more recently deep learning-based approaches\cite{torfi2020natural, li2020survey}. Our focus here is more on the recent deep learning methods. \cite{devlin2018bert, peters2018deep, akbik2018contextual} propose contextualized word/string representations to better model sentence semantics thus improving NER performance. Deep learning systems are however limited by the amount of labeled training data. Hence, there have been efforts to generate noisy labeled data as weak supervision signals for training. \cite{shang2018learning} propose AutoNER framework which uses distant supervision for generating noisy labels for training. \cite{arora2017extracting} use regular expression patterns for artificial training data generation and train a LSTM model for entity extraction. \cite{zhou2019dual} propose adversarial perturbations and use CNN-LSTM-CRF\cite{ma2016end} architecture to train a robust model with limited gold data. \cite{liu2018empower, liu2018efficient} train a task-aware language model from unlabeled data which guides NER. As a side note, sometimes gold labels in datasets may have some errors. \cite{wang2019crossweigh} use a bootstrapping framework to correct such imperfect annotations.

In scientific and biomedical domain, NER has its own set of challenges. Entity mentions are long and may have alpha-numeric symbols and chemical formulas which may be hard for a language model to make sense of. Many entity types also low-resource with a shortage of labelled training data. \cite{wang2019cross} use a multi-task learning framework and combine labelled data from multiple corpora mapping entity tags across corpora to coherent classes. \cite{wang2020pattern} use noisy distant supervision from domain-specific dictionaries. \cite{wang2018penner} form entity-type meta patterns for entity extraction. \cite{wang2019distantly} make use of a setup similar to AutoNER\cite{shang2018learning} tailor-made for biomedical NER. \cite{wang2020fine, wang2020comprehensive} apply weak or distant supervision for NER on COVID-19 literature.

Several named entities have numeric nature, for example, phone numbers and SSNs look very much alike and require understanding of number patterns to differentiate the two. NER systems dealing with such entities need some explicit numeral semantic handling. \cite{chen2019numeracy} propose a Bi-GRU framework for understanding numbers. \cite{wallace2019nlp} shows that standard contextual word embeddings like ELMo\cite{peters2018deep}, BERT\cite{devlin2018bert} have some good sense of numbers and character-level embeddings are more effective than word-level embeddings in capturing numeral semantics.
