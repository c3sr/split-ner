Concretely, in named entity recognition (NER), we are given an unlabeled raw text and the goal is to identify certain entities (sequence of tokens) of interest. In this study, we approach this task using machine learning in supervised learning setting. The idea is, we are given a set of labeled sentences with the required entities marked. We feed those samples to a machine learning system to learn from it. The system is then evaluated on a manually labeled test set and relevant evaluation metrics are reported. In this chapter, we will look at various ways of approaching NER along with detailed ablation studies and variants. Simultaneously, we compare our results with existing published state-of-the-art approaches on multiple datasets.

\section{Sequence Labeling}
Traditionally, the most intuitive way of approaching named entity recognition is as a sequence labeling task. An input sentence can be considered as a sequence of \texttt{n} tokens, fed to a neural network model. For each token, the model classifies it into an output class as per \texttt{BIO} tagging scheme. As the model backbone, we use \texttt{CNN-LSTM-CRF} and \texttt{BERT} architectures as described below.

\begin{itemize}
    \item \texttt{CNN-LSTM-CRF}: As proposed in \cite{ma2016end}, this is a popular NER model. The character-level CNN helps capture the intrinsic patterns and word-level semantics of individual tokens. These character-level embeddings are concatenated with word embeddings and fed to bidirectional LSTM\cite{} which helps capture the sentence grammar and token inter-dependencies. Finally, the CRF models the output tag sequence and makes sure that abrupt and unexpected tag transitions do not take place. 
    
    \item \texttt{BERT}: Proposed in \cite{devlin2018bert}, BERT is a bidirectional encoder implemented using the transformer architecture \cite{}. The input is a sentence, which is broken down into sub-words/sub-tokens. Multi-head attention and several layers of encoders are able to capture long-term relationships among tokens and semantics well. Finally, the model outputs contextualized embeddings for each sub-token in the sentence. Then we have a simple fully-connected layer followed by Softmax to classify each token into an output class. The model is optimized using cross-entropy loss.
\end{itemize}

\subsection{Experiment Details}
For English news corpora like \texttt{CoNLL 2003} and \texttt{OntoNotes 5.0}, we use \texttt{bert-base-uncased} model from \texttt{transformers}\footnote{https://huggingface.co/transformers} python package in \texttt{PyTorch}. For biomedical datasets, we use \texttt{BioBERT-Base}\footnote{https://github.com/dmis-lab/biobert#download} model which is specifically pretrained on biomedical data. For \texttt{CNN-LSTM-CRF} framework, as word embeddings we use the contextual embeddings from BERT/BioBERT model but freeze the BERT architecture for training. So, the trainable model architecture still remains a core \texttt{CNN-LSTM-CRF}. We take the mean of sub-token embeddings from BERT to get the embedding for the token.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG} & \textbf{JNLPBA} & \textbf{CoNLL 2003} & \textbf{OntoNotes 5.0}\\\hline
	\texttt{BERT-Freeze} & 75.42 & todo & todo & todo \\\hline
	\texttt{CNN-LSTM-CRF (BERT-Freeze)} & 84.1 & 76.2 & 91.6 & 86.4 \\\hline
	\texttt{BERT} & 85.99 & 74.35 & 91.36 & 83.39 \\\hline
	\end{tabular}
    \caption{Results: Sequence Labeling (Test set Micro-F1 in \%)}
    \label{tab:res_seq_labeling}
\end{table}

\subsection{Observations}
Based on results summarized in Table \ref{tab:res_seq_labeling}:
\begin{itemize}
    \item Pretrained BERT embeddings themselves capture linguistic semantics well as can be seen from \texttt{BERT-Freeze}.
    \item Fine-tuning the BERT model or using BERT embeddings with CNN-LSTM-CRF both work well and give almost comprable enhancements over the \texttt{BERT-Freeze} setup.
\end{itemize}

\section{Question Answering}

% QA3, QA4, Where
The NER task can also be modelled as a question answering problem where, we give a question to the model asking it to extract the entity of interest from the supplied text. 

\cite{li2019unified} and \cite{li2019dice} show the effectiveness of this setup using a simple BERT model architecture on multiple general English and Chinese news datasets. They make the model output candidate spans (start and end indices) where the entity in question is present. This setup has advantages over the naive sequence labeling setup since using this framework we can extract even nested or overlapping entities while sequence labeling can capture only flat non-overlapping entities. Since they output spans, so their classification layer does a $\mathcal{O}(n^2)$ computation where $n$ is the number of tokens in the input sentence. \cite{banerjee2019knowledge} use a similar setup and output simple \texttt{B}, \texttt{I} and \texttt{O} tags for each token to mark the presence of the entity in question. Hence, their problem complexity becomes $\mathcal{O}(n)$. On top of this setup, we study the variations described below.

\begin{itemize}
    \item \textbf{Tagging Scheme}: Classify each token into 3 output classes (\texttt{B}, \texttt{I} and \texttt{O}) [\texttt{BERT-QA(BIO)} model] or 4 classes explicitly modeling the end boundary using \texttt{E} output class [\texttt{BERT-QA(BIOE)} model]. In both these models, we ask questions of the form, \textit{What is the person mentioned in the text?}.
    
    \item \textbf{Question Formulation}: \cite{banerjee2019knowledge} show that their trained model gives a high importance to the question word. Hence we probe the model and change the question slightly to observe its impact. Instead of asking \textit{What} [\texttt{BERT-QA(What)} model], we ask \textit{What is the person mentioned in the text?} [\texttt{BERT-QA(Where)} model]. In both these models, we follow the \texttt{BIOE} output tagging scheme.
\end{itemize}

\subsection{Experiment Details}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG} & \textbf{JNLPBA} & \textbf{CoNLL 2003}\\\hline
	\texttt{BERT-QA(BIO)} & todo & 74.81 & todo\\\hline
	\texttt{BERT-QA(BIOE)} & 86.45 & todo & 91.17\\\hline
	\end{tabular}
    \caption{Results: Question Answering (Tagging Scheme) (Test set Micro-F1 in \%)}
    \label{tab:res_qa_tagging}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG} & \textbf{JNLPBA} & \textbf{CoNLL 2003}\\\hline
	\texttt{BERT-QA(What)} & 86.45 & todo & 91.17\\\hline
	\texttt{BERT-QA(Where)} & \textbf{86.83} & 74.64 & \textbf{91.82}\\\hline
	\end{tabular}
    \caption{Results: Question Answering (Question Formulation) (Test set Micro-F1 in \%)}
    \label{tab:res_qa_question}
\end{table}

\subsection{Observations}
\begin{itemize}
    \item From Table \ref{tab:res_qa_tagging}, \texttt{BIOE} tagging is able to better capture the entity boundaries as compared to \texttt{BIO} tagging scheme.
    
    \item From Table \ref{tab:res_qa_question}, we observe that the model is sensitive to slight changes in question semantics. Asking a \textit{where} to the model helps it learn and identify entities better than asking \textit{what}. 
\end{itemize}

\section{Span Detection and Classification Pipeline}
Another way of approaching NER is to break it down into a two-step pipelined procedure. In the first step, given a sentence, we detect all entity spans (Span Detector). In the next step, these spans are classified into an output entity class by another model (Span Classifier).

\subsection{Experiment Details}

\begin{itemize}
    \item \textbf{Span Detection}: We treat this sub-problem as a question answering task. Every sample sentence of the form, \textit{Emily}[\texttt{PERSON}] \textit{lives in United States}[\texttt{LOCATION}], is converted into the form, \textit{What is the entity mentioned in the text? Emily lives in United States}. This is fed to BERT (\texttt{bert-base-uncased}) model and the outputs are passed through a single fully connected layer followed by softmax. The model is expected to output \texttt{B}, \texttt{I} and \texttt{O} tags for each token and in this case, detect two spans, \textit{Emily} and \textit{United States}. 
    
    \item \textbf{Span Classification}: Again, we treat this sub-problem as a question answering task as well. For every gold labeled entity mention in a training set sentence, \textit{Emily}[\texttt{PERSON}] \textit{lives in United States}[\texttt{LOCATION}], we form a sample, \textit{Emily lives in United States. What is Emily?} The sentence is passed to BERT (\texttt{bert-base-uncased}) and the final pooled output for the sentence is fed to a fully connected layer followed by softmax to classify the sentence into one of the entity types, in this case, \texttt{PERSON}.
    
    \item \textbf{Pipeline}: During evaluation, every unlabeled sentence is passed through Span Detector and for each output span, we convert to an input sample for Span Classifier.
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
	\textbf{} & \textbf{BIONLP13CG} & \textbf{JNLPBA} & \textbf{CoNLL 2003}\\\hline
	\texttt{Span Detection} & 90.12 & 78.35 & 95.23\\\hline
	\texttt{Span Classification} & 94.06 & 95.08 & 94.50\\\hline
	\texttt{Pipeline} & 85.89 & 75.01 & 91.64\\\hline
	\texttt{BERT-QA} & 86.45 & todo & 91.17\\\hline
	\end{tabular}
    \caption{Results: Span Pipeline (Test set Micro-F1 in \%)}
    \label{tab:res_span}
\end{table}

\subsection{Observations}
Table \ref{tab:res_span} reports the results of the pipelined span detection and classification procedure and also presents comparison with simple BERT question answering setup for NER. We present this comparison since question answering model serves as the primary backbone of our current span-based extraction procedure. Note that for one-to-one comparison all results here correspond to \texttt{B}, \texttt{I}, \texttt{O} output tagging and \textit{What} as question word in question formulation.

\begin{itemize}
    \item \textbf{Span Detection}: Detecting all entity spans together without classification is a simpler problem for the model than full NER and hence we get better performance compared to \texttt{BERT-QA} model.
    
    \item \textbf{Span Classification}: Given that spans are detected correctly, this second step of the pipeline is relatively simple for the BERT model. On all datasets, we see above 90\% Micro-F1 on test set.
    
    \item \textbf{Pipeline}: The pipelined procedure gives comparable results to standard question answering based NER model. The main bottleneck lies in the span detection part. Since this procedure is pipelined, errors in first step propagate to the next step leading to an overall reduced performance.
\end{itemize}

\section{Learning Objective Variations}
An ML algorithm learns by optimizing its learning objective. For named entity recognition in supervised learning setup, we get a corpus labeled with gold entity mentions. As highlighted in section \ref{sec:nature_of_entities}, there may be an inherent labeling bias in the dataset. Some entities have more representation, more labeled mentions (\textit{high-resource}) while others are rare entities (\textit{low-resource}). With fewer samples, it becomes difficult for the model to learn good differentiating rules for extracting the low-resource entities. To handle this bias in dataset, it is common to give more importance/weight to low-resource entity samples in calculating the learning objective and optimizing on it. In this direction, we experiment with the variants detailed below.

\begin{itemize}
    \item \textbf{Cross-Entropy (CE) Loss}: This is the standard objective function in which we calculate the cross entropy between the predicted and gold labels for each token in the sentence.
    
    \item \textbf{Weighted Cross-Entropy Loss}: Here, based on the gold-labeled tag for a token we assign a weight to its contribution. All tokens which belong to a valid entity span contribute equally to the loss and all other tokens contribute with a reduced weight of 0.5. This helps because a large part of the corpus consists of tokens which belong to the \texttt{O} tag category. For instance, the \texttt{BioNLP13CG} corpus has 76.5\% tokens with \texttt{O} tag.
    
    \item \textbf{Punctuation Weighted CE Loss}: From qualitative analysis of misclassified samples in standard CE loss setup we noticed that the model is not able to learn good representations for special symbols like parenthesis, hyphen, period, slash etc. Hence, we force the model to learn these symbols well by penalizing the model twice if the misclassified token is a punctuation/special symbol.
    
    \item \textbf{Dice Loss}: As proposed by \cite{li2019dice}, we address the above mentioned data imbalance issue between entity and non-entity tokens using dice loss, based on S{\o}rensen-Dice coefficient\cite{} or Tversky index\cite{}. This helps because standard CE loss in accuracy-oriented while during evaluation we calculate F1-measure. The dice loss gives equal importance to false-positives and false-negatives at training time and hence reduces the discrepancy among these training and test time metrics.
    
    \item \textbf{CRF}: Apart from the cross-entropy objective, we also experiment by adding a CRF layer on the fully connected layer output in standard BERT-based model. CRF layer is found to work well with bidirectional LSTM\cite{ma2016end} in modeling output tag transitions and emissions.
    
\end{itemize}

\subsection{Experiment Details}
For CRF implementation, we use \texttt{torchcrf} python package.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
	\textbf{} & \textbf{BIONLP13CG} & \textbf{CoNLL 2003}\\\hline
	\texttt{CE Loss} & 85.99 & 91.36\\\hline
	\texttt{Weighted CE Loss} & 85.93 & todo\\\hline
	\texttt{Punctuation CE Loss} & 86.12 & todo\\\hline
	\texttt{Dice Loss} & 86.35 & 90.76\\\hline
	\texttt{CRF} & 86.20 & todo\\\hline
	\end{tabular}
    \caption{Results: Learning Objectives (Test set Micro-F1 in \%)}
    \label{tab:res_loss}
\end{table}

\subsection{Observations}
\begin{itemize}
    \item \texttt{Weighted CE Loss} is found to perform comparable to \texttt{CE Loss} and both do not perform as well as the other variants.
    
    \item In \texttt{BioNLP13CG} corpus, \texttt{Dice Loss} performs well as this corpus as several high and low resource entities (data imbalance). \texttt{Dice Loss} is not able to give its advantages with \texttt{CoNLL 2003} corpus since here all 4 entity types have a comparable and high representation.
    
    \item \texttt{Punctuation CE Loss} is able to make the model learn slightly better semantics of special symbols and hence performs better than the standard \texttt{CE Loss} counterpart.
    
    \item \texttt{CRF} is able to capture the output tag transitions better and hence performs better than standard \texttt{CE Loss} setting.
\end{itemize}

\section{Capturing Additional Token Semantics}
\label{sec:additional_token_semantics}

Both in \texttt{CNN-LSTM-CRF} and transformer-based architectures, we rely on on a pretrained BERT model. For biomedical datasets we use BioBERT which is trained on scientific and biomedical literatures while for English news datasets, we work with \texttt{bert-base-uncased} model which is trained on Wikipedia and books. BERT models rely on Word-Piece tokenizer\cite{} which considers breaking words into sub-words for representation. Even then from qualitative analysis we find that there are several terms whose semantics are not captured well by the existing BERT model. This causes errors which can be categorized as:

\begin{itemize}
    \item \textbf{Out of Vocabulary Tokens}: News articles and scientific texts both sometimes make use of abbreviations or localized entities which may be specific to that news article, event time or research paper, but are rare otherwise. Additionally biomedical texts also consist of chemical names in scientific form which includes numerals etc. which need to be extracted. Semantics of such terms is not captured well by generically pretrained models. Table \ref{tab:oov_issue} shows some examples from \texttt{BioNLP13CG} corpus.
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}\hline
    	\textbf{Entity} & \textbf{Misclassification Examples}\\\hline
    	\texttt{Gene\_or\_Gene\_Product} & DPD, Xhol, mutCK1delta, FAS\\\hline
    	\texttt{Simple\_Chemical} & MnCl2, AglRhz, NO\\\hline
    	\texttt{Cell} & LoVo, DeltaG45, BMSVTs\\\hline
    	\texttt{Amino\_Acid} & phosphoS727, Y705F\\\hline
    	\end{tabular}
        \caption{Out-of-Vocabulary tokens in \texttt{BioNLP13CG} corpus}
        \label{tab:oov_issue}
    \end{table}
    
    \item \textbf{Special Symbols}: Several entities to be extracted have hyphens, periods, parenthesis within them which are not captured well by pretrained BERT model leading to partial entity detection. Table \ref{tab:boundary_issue} gives some examples from \texttt{BioNLP13CG} corpus.
    
    \item \textbf{Modifier Suffix/Prefix}: Apart from the root entity required to be extracted the gold labels sometimes expect a modifier term as well which occurs as a prefix/suffix. Missing these again leads to boundary detection issues. Table \ref{tab:boundary_issue} gives some examples from \texttt{BioNLP13CG} corpus.
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}\hline
    	\textbf{Misclassification Category} & \textbf{Gold} & \textbf{Predicted}\\\hline
    	\texttt{Special Symbols} & L . Se ( + ) cells & L . Se\\\hline
    	\texttt{Special Symbols} & Gs - IB ( 4 - ) ion & Gs - IB ( 4\\\hline
    	\texttt{Modifier Suffix/Prefix} & epicardial coronary artery & coronary artery\\\hline
    	\texttt{Modifier Suffix/Prefix} & T140 analogs & T140\\\hline
    	\end{tabular}
        \caption{Boundary detection issues in \texttt{BioNLP13CG} corpus}
        \label{tab:boundary_issue}
    \end{table}
\end{itemize}

\subsection{Experiment Details}

To address the above mentioned issues, we provide additional inputs and infrastructure to the model to learn the underlying semantics better.

\begin{itemize}
    \item \textbf{Special Symbol Features}: Before feeding the output of BERT model to final fully-connected classification layer, we add an extra one-hot dimension which is set if the current input token is a pure special symbol. This means we assign \texttt{1} for \textit{hyphen}(\texttt{-}), \textit{parenthesis}(\texttt{(} and \texttt{)}), \textit{comma}(\texttt{,}) etc. while assign \texttt{0} for tokens like \texttt{carbon}, \texttt{123}, or mixed format tokens like \texttt{Ca(2+)}, \texttt{AB-3} etc.
    
    \item \textbf{Word Type Features}: As an extension of the above special symbol features, here we associate each token with a word type as shown in Table \ref{tab:word_type_encoding} which is converted into a one-hot vector concatenated with BERT output embeddings before sending to final classifier layer.
    
    \item \textbf{Character and Pattern Features}: Chemical formulas and scientific names generally follow a nomenclature convention or pattern. Similarly, out-of-vocabulary tokens also may have some intrinsic character level information which is not well captured by the BERT model which considers sub-words. Infact this issue with BERT is studied well in \cite{boukkouri2020characterbert} which propose to use a character CNN instead of word-piece tokenizer at the stating stage. Motivated by \texttt{CNN-LSTM-CRF} setup and this study, we do the following:
    
    \textbf{Modeling characters}. Each word is passed to BERT and in parallel, to five 1-dimensional CNNs with kernel sizes of 1 to 5, each having 16 input and 16 output channels. Input character is indexed and embedded into 16-dimensions through an embedding layer. The CNN outputs are concatenated and passed through a linear layer to get overall 768-dimensional output vector for each token.
    
    \textbf{Modeling patterns}. Each word is converted to a pattern (like regular expression, a denser space than simply all characters) converting all uppercase letters to \texttt{U}, lowecase to \texttt{L}, digits to \texttt{D} etc. and then sent to a separate character CNN (like the one described above) and then to a bidirectional LSTM to get contextual token embeddings.
    
    Finally, these character and pattern embeddings are concatenated with BERT outputs and fed to final classifier layer for tag classification.
    
    \item \textbf{Part-of-Speech and Dependency Parse Features}: Concatenate BERT embeddings with the one-hot POS and one-hot dependency parse features before feeding to final classifier layer.
    
    \item \textbf{Head Tokens}: BERT uses word-piece tokenizer and hence can break a given single token into multiple sub-words. Instead of doing a token classification on each of these sub-words and making sure everything is correct, it is simpler to take the embedding output for the first sub-word (\textit{head}) for each token. This technique is also used in the original BERT paper \cite{devlin2018bert} for their NER experiments. 
    
    \item \textbf{Highway Network}: Instead of using a simple concatenation of character features with BERT as done in previous experiment, here we create a highway network\cite{}, similar to the one used in BiDAF\cite{} architecture in which we train a gated network to learn when to use BERT vectors and when to use the additional character-level information.
\end{itemize}

\subsection{Observations}


\section{Training Effectiveness Study}
In the previous section \ref{sec:additional_token_semantics}, we mostly try to give additional information to a BERT architecture by concatenating BERT outputs with some additional feature vectors before final classification. Giving additional information to a model is one thing but whether the model is actually able to pick cues from the additional information or not, is another. In this section, we study the training effectiveness on \texttt{BioNLP13CG} dataset in \texttt{Sequence Labeling} setting using \texttt{BioBERT-Base} model.

\subsection{Feeding Answer as Input}

To study the training effectiveness, we give the model the best ideal-case information, i.e. for each token, we feed its gold label as a one-hot vector. This is concatenated with BERT output before feeding to final classifier. We study the following variants:

\begin{itemize}
    \item \texttt{BERT(Freeze) + Answer}: We freeze the BERT model parameters and concatenate the answer vectors. This effectively reduces the no. of trainable parameters to a few thousand. We try this in two settings, low learning rate of $10^{-5}$ (recommended BioBERT learning rate\footnote{https://github.com/dmis-lab/biobert#named-entity-recognition-ner} and high learning rate of $0.005$.
    
    \item \texttt{BERT + Answer}: This mimics the real-world scenario where the BERT model is fine-tuned with given additional information at a low learning rate of $10^{-5}$.
\end{itemize}

From the results summarized in Table \ref{}, we observe that with a very low learning rate of $10^{-5}$, the model is not able to learn from the additional answer information provided (both with frozen BERT and otherwise). At a higher learning rate of $0.005$, the model catches the provided cue well.

\subsection{What happens at high learning rate?}

From the previous section, we see that we need a relatively higher learning rate to learn from additional information fed to the model. But with BERT fine-tuning, it is recommended to use a very low learning rate in the order of $10^{-5}$. To study this, we pass answer spans as input to the model. Basically, for each token, we add a one-hot dimension which is set if the token belongs to a valid entity else it is set to $0$. This means the model is actually being fed the gold spans and all it has to do is span classification. This should be relatively easy and give good results as seen previously in Section \ref{}. We experiment with the following variants:

\begin{itemize}
    \item \texttt{BERT (Freeze)}: Freezing BERT model and passing through a single trainable classifier layer. This model has only around $26,000$ trainable parameters. 
    
    \item \texttt{BERT (Freeze) + Gold Span}: Same as the above but with gold spans given as additional input. So, this model is expected to perform better than \texttt{BERT (Freeze)}.
    
    \item \texttt{BERT + Gold Span}: Same as the above model but with trainable BERT model. We try training this in two different settings, with low learning rate of $10^{-5}$ and high learning rate of $0.005$.
    
    \item \texttt{BERT}: This is the standard BERT model with final classification layer and no additional feature inputs fine-tuned at learning rate of $10^{-5}$.
\end{itemize}

From Table \ref{}, we make the following observations:

\begin{itemize}
    \item From \texttt{BERT (Freeze)} and \texttt{BERT (Freeze) + Gold Span}, we observe that indeed giving gold span information helps the model.
    
    \item From \texttt{BERT(Freeze) + Gold Span} and \texttt{BERT + Gold Span (LR: $10^{-5}$)}, we observe that indeed fine-tuning is able to learn the semantics of entities much better than when using the BERT embeddings right out-of-the-box.
    
    \item However, from \texttt{BERT} and \texttt{BERT + Gold Span (LR: $10^{-5}$)}, we observe that with a low learning rate, the model is not able to focus on and effectively utilize the gold span information.
    
    \item Finally, from \texttt{BERT + Gold Span (LR: $10^{-5}$)} and \texttt{BERT + Gold Span (LR: $0.005$)}, we observe that increasing the learning rate has a deteriorating effect on the pretrained BERT parameters and the rigorous push from high learning rate pushes the model to an unsatisfactory local optima.
\end{itemize}

\section{Tagging Scheme Variation}
As described in Section \ref{sec:tagging_scheme}, in this section, we study how much impact do different output tagging schemes have on boundary detection and learning of the model. We test the \texttt{2-Tag}, \texttt{BIO} and \texttt{BIOE} tagging schemes in question answering setup on \texttt{BIONLP13CG} corpus.

We omit the experiments with sequence labeling setup since for \texttt{K} output tags, \texttt{2-Tag} scheme gives \texttt{K + 1} output tags (\texttt{+1} for \texttt{O} tag). With \texttt{BIO} scheme, we have \texttt{2K + 1} output tags and with \texttt{BIOE} scheme, we have \texttt{3K + 1} tags. For \texttt{BIONLP13CG} corpus which already has \texttt{K = 16}, the \texttt{BIOE} scheme makes number of output tags as \texttt{49}, which is too large to train well since we don't have enough training data. However, for the question answering setup, the number of output tags remains \texttt{3} for \texttt{2-Tag}, \texttt{4} for \texttt{BIO} and \texttt{5} for \texttt{BIOE} scheme which is manageable.

From results in Table \ref{}, we observe that as expected, explicitly modeling the begin and end boundaries performs the best while not modeling start and end at all, in \texttt{2-Tag} scheme performs the least among them. However this will have lesser number of parameters to train and more representative samples for each case.

\section{Pretrained Model Variation}


\section{Clustering and Segregation of Diverse Entities}
In this section, we focus on a dataset with known data imbalance issues (presence of high and low-resource entities) like \texttt{BIONLP13CG} dataset.

\section{Nested Entities and Supplying Additional Context}

\section{Comparative Precision/Recall Analysis}