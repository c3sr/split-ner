We approach the named entity recognition problem from different perspectives, as a sequence labeling task, a question answering task and a span detection and classification task. In this chapter, we look at all these in detail and compare and contract them with one-another. Primarily, we develop on top of pretrained BERT model. We look at the effect of loss function, tagging schemes, providing additional semantic information etc. on the overall task. We also study how much guidance the model derives from any additional information fed with the input. Experiments are conducted on multiple datasets and we simultaneously compare our models with previously published state-of-the-art results as well. 

\section{Sequence Labeling}
\begin{definition}
\label{def:seq_labeling}
    Given sentence $\mathcal{S}$ as a $N$-length sequence of tokens, $\mathcal{S} = \langle w_1, w_2 \ldots w_N \rangle$, sequence labeling is the task of forming output sequence $\mathcal{L} = \langle l_1, l_2 \ldots l_N \rangle$ where $l_i$ is the label assigned to token $w_i$. 
\end{definition}

For NER, we extract entity mentions by making output labels follow a tagging scheme (Section \ref{sec:tagging_scheme}). For example, with \texttt{BIO} scheme, each output label is of the form \texttt{B-Tag}, \texttt{I-Tag} or \texttt{O} with $\texttt{Tag} \in \mathcal{T}$ where $\mathcal{T}$ is the set of all possible entity types. For this task, we use two deep neural architectures:

\begin{itemize}

    \item \texttt{BERT}: Proposed by \cite{devlin2018bert}, BERT is a bidirectional encoder transformer\cite{vaswani2017attention}. It applies WordPiece\cite{wu2016google} tokenization on input sentence which is then passed through several encoder layers with multiple attention heads capturing sentence semantics and inter-token relationships well. The model outputs contextualized embeddings for each sub-token in the sentence. We take the last hidden layer outputs from BERT model and pass it to a fully connected layer. The outputs are converted to a probability distribution over labels space. Model parameters are initialized from a pretrained model and fine-tuned on our NER task.
    
    \item \texttt{CNN-LSTM-CRF (BERT-Freeze)}: Proposed by \cite{ma2016end}, this model has three components. A CNN layer helps capture intrinsic character-level semantics and patterns. These features are then concatenated with word embeddings and fed to a bidirectional LSTM\cite{hochreiter1997long} which captures sentence grammar and token inter-dependencies. Finally, a CRF\cite{lafferty2001conditional} layer on top helps make sure that output label sequence does not contain abrupt and unexpected transitions. We use the contextual word embeddings from pretrained BERT model as word embeddings fed to LSTM but freeze BERT parameters during training. This makes sure that ultimate model being trained remains CNN-LSTM-CRF.
    
    \item \texttt{BERT-Freeze}: To understand how much semantic information is already captured in a pretrained BERT model, we use the exact same architecture as \texttt{BERT} model above but freeze the BERT model parameters. So, the only trainable parameters remain from the fully connected layer.
\end{itemize}

\subsection{Experiment Details}
For general English corpora like \texttt{CoNLL 2003} and \texttt{OntoNotes 5.0}, we use pretrained \texttt{bert-base-uncased} model from \texttt{transformers}\footnote{https://huggingface.co/transformers} python package in \texttt{PyTorch}. For biomedical datasets, we use \texttt{BioBERT-Base}\footnote{https://github.com/dmis-lab/biobert\#download} model. For \texttt{CNN-LSTM-CRF (BERT-Freeze)} setup, we take the mean of sub-token embeddings from BERT to get the embedding for each input token. We make use of \texttt{BIO} tagging scheme in all our sequence labeling experiments. Models with frozen BERT parameters used learning rate $0.005$. CNN input channels is set to character vocabulary size and each character is fed as a one-hot vector. CNN has $128$ output channels, kernel size of $5$ and does a max pooling to get word-level representation from character-level outputs. LSTM hidden dimensions is set to $256$.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG} & \textbf{JNLPBA} & \textbf{CoNLL 2003} & \textbf{OntoNotes 5.0}\\\hline
	\texttt{BERT-Freeze} & 75.42 & 55.93 & 82.79 & 67.35 \\\hline
	\texttt{CNN-LSTM-CRF (BERT-Freeze)} & 84.1 & 76.2 & 91.6 & 86.4 \\\hline
	\texttt{BERT} & \textbf{85.99} & 74.35 & 91.36 & 83.39 \\\hline
	\end{tabular}
    \caption{Results: Sequence Labeling (Test set Micro-F1 in \%)}
    \label{tab:res_seq_labeling}
\end{table}

\subsection{Observations}
Based on results summarized in Table \ref{tab:res_seq_labeling}:
\begin{itemize}
    \item \texttt{BERT-Freeze} serves as a naive baseline for the other two language models to improve upon.
    \item Both CNN-LSTM-CRF and transformer-based BERT architecture perform comparably on NER task on these relatively small-sized datasets as can be seen from \texttt{BERT} and \texttt{CNN-LSTM-CRF (BERT-Freeze)} results.
\end{itemize}

\section{Question Answering}
\label{sec:question_answering}
In recent years there has been a trend of formulating NER problems as question answering(QA) tasks. \cite{li2019entity, levy2017zero} model relation extraction as QA tasks and \cite{mccann2018natural} propose a QA-based multi-task learning setup. 

For NER using a QA setup, we feed a question to the model asking it to extract all mentions of a given entity type from the supplied text. Since there can be multiple entities of interest, each combination of entity question and input sentence is fed to the model. \cite{li2019unified, li2019dice} show the effectiveness of such a setup using BERT over multiple general English and Chinese news datasets and output candidate spans (start and end indices) where the entity in question is present. This setup has advantages over sequence labeling since it can handle nested entities as well. 

However, for learning spans (start and end indices), the classification layer has to do $\mathcal{O}(n^2)$ computations where $n$ is the number of tokens in the input sentence. Such a computation can be expensive for large sentences. To mitigate this, \cite{banerjee2019knowledge} propose an approach in the middle of question answering and sequence labeling. They return \texttt{B}, \texttt{I} and \texttt{O} labels for each token to mark the presence of the entity in question. Hence, their problem complexity becomes $\mathcal{O}(n)$ since they output one label for each token. We implement this framework and study the effect of several factors:

\begin{itemize}
    \item \textbf{Tagging Scheme}: Classify each token using \texttt{BIO} or \texttt{BIOE} scheme. In both these models, we ask questions of the form, \textit{What is the person mentioned in the text?}.
    
    \item \textbf{Question Formulation}: In QA setup, extraction from a sentence is highly dependent on the question semantics. We study the importance of the question word being used (\texttt{What} or \texttt{Where}). Our sample question is of the form, \textit{\texttt{What}$\vert$\texttt{Where} is the person mentioned in the text?} In both these models, we follow the \texttt{BIOE} output tagging scheme.
    
    \item \textbf{Entity Scrambling}: In the question, \textit{What is the \texttt{entity} mentioned in the text?}, we study the effect of the entity keyword used. We replace each entity with some scrambled English letters, for example, \texttt{Person} becomes \texttt{xyz12qqr}. So, during training, we ask questions like \textit{What is the \texttt{xyz12qqr} mentioned in the text?} but give the right \texttt{person} mentions as gold labels to the model. Since our task here is to probe the model and develop an understanding of what it focuses on, so we conduct this experiment on a single dataset, \texttt{BIONLP13CG}. Table \ref{tab:entity_scramble_bio} shows some example scrambled entity keywords. We compare it with the unscrambled model in the same \texttt{BIOE} tagging scheme setting.
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}\hline
    	\textbf{Original Entity Name} & \textbf{Scrambled Entity Name}\\\hline
    	\texttt{Cancer} & \texttt{OUYOFhok}\\\hline
    	\texttt{Amino\_acid} & \texttt{DJHkjh KJDSjh}\\\hline
    	\texttt{Organ} & \texttt{UQUIhkjsndf}\\\hline
    	\texttt{Cell} & \texttt{OIFoisjf}\\\hline
    	\end{tabular}
        \caption{Entity Scrambling Examples}
        \label{tab:entity_scramble_bio}
    \end{table}
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG} & \textbf{JNLPBA} & \textbf{CoNLL 2003}\\\hline
	\texttt{BERT-QA(BIO)} & 86.15 & 74.81 & 91.06\\\hline
	\texttt{BERT-QA(BIOE)} & \textbf{86.45} & \textbf{74.92} & \textbf{91.17}\\\hline
	\end{tabular}
    \caption{Results: Question Answering (Tagging Scheme) (Test set Micro-F1 in \%)}
    \label{tab:res_qa_tagging}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG} & \textbf{JNLPBA} & \textbf{CoNLL 2003}\\\hline
	\texttt{BERT-QA(What)} & 86.45 & \textbf{74.81} & 91.17\\\hline
	\texttt{BERT-QA(Where)} & \textbf{86.83} & 74.64 & \textbf{91.82}\\\hline
	\end{tabular}
    \caption{Results: Question Answering (Question Formulation) (Test set Micro-F1 in \%)}
    \label{tab:res_qa_question}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG}\\\hline
	\texttt{Original} & \textbf{86.45}\\\hline
	\texttt{Scrambled} & 85.83\\\hline
	\end{tabular}
    \caption{Question Answering (Entity Scrambling) (Test set Micro-F1 in \%)}
    \label{tab:res_qa_scrable}
\end{table}

\subsection{Observations}
\begin{itemize}
    \item From Table \ref{tab:res_qa_tagging}, \texttt{BIOE} tagging is able to better capture the entity boundaries as compared to \texttt{BIO} tagging scheme.
    
    \item From Table \ref{tab:res_qa_question}, we observe that the model is sensitive to slight changes in question semantics. Since the ultimate goal of the model is to identify mention boundaries, asking \texttt{Where} works better than \texttt{What}. 
    
    \item From Table \ref{tab:res_qa_scrable}, we observe that as expected, \texttt{Original} keyword model works better than \texttt{Scrambled}. However the contribution of entity keyword is very minimal. It is possible to form some logical groups of entity mentions with unknown group name and the model should still be able to distinguish and differentiate the group well. We use this finding in Section \ref{sec:clustering}.
\end{itemize}

\section{Span Detection and Classification Pipeline}
\label{sec:span_pipeline}

Another way of approaching NER is to break it down into a two-step pipelined procedure. In the first step, given a sentence, we detect all entity spans (Span Detector). In the next step, these spans are classified into an output entity class by another model (Span Classifier).

\subsection{Experiment Details}

\begin{itemize}
    \item \textbf{Span Detection}: We treat this sub-problem as a question answering task. Every sample sentence of the form, \textit{Emily}[\texttt{PERSON}] \textit{lives in United States}[\texttt{LOCATION}], is converted into the form, \textit{What is the entity mentioned in the text? Emily lives in United States}. This is fed to BERT (\texttt{bert-base-uncased}) model and the outputs are passed through a single fully connected layer followed by softmax. The model is expected to output \texttt{B}, \texttt{I} and \texttt{O} tags for each token and in this case, detect two spans, \textit{Emily} and \textit{United States}. 
    
    \item \textbf{Span Classification}: Again, we treat this sub-problem as a question answering task as well. For every gold labeled entity mention in a training set sentence, \textit{Emily}[\texttt{PERSON}] \textit{lives in United States}[\texttt{LOCATION}], we form a sample, \textit{Emily lives in United States. What is Emily?} The sentence is passed to BERT (\texttt{bert-base-uncased}) and the final pooled output for the sentence is fed to a fully connected layer followed by softmax to classify the sentence into one of the entity types, in this case, \texttt{PERSON}.
    
    \item \textbf{Pipeline}: During evaluation, every unlabeled sentence is passed through Span Detector and for each output span, we convert to an input sample for Span Classifier.
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
	\textbf{} & \textbf{BIONLP13CG} & \textbf{JNLPBA} & \textbf{CoNLL 2003}\\\hline
	\texttt{Span Detection} & 90.12 & 78.35 & 95.23\\\hline
	\texttt{Span Classification} & 94.06 & 95.08 & 94.50\\\hline
	\texttt{Pipeline} & 85.89 & 75.01 & 91.64\\\hline
	\texttt{BERT-QA} & 86.45 & todo & 91.17\\\hline
	\end{tabular}
    \caption{Results: Span Pipeline (Test set Micro-F1 in \%)}
    \label{tab:res_span}
\end{table}

\subsection{Observations}
Table \ref{tab:res_span} reports the results of the pipelined span detection and classification procedure and also presents comparison with simple BERT question answering setup for NER. We present this comparison since question answering model serves as the primary backbone of our current span-based extraction procedure. Note that for one-to-one comparison all results here correspond to \texttt{B}, \texttt{I}, \texttt{O} output tagging and \textit{What} as question word in question formulation.

\begin{itemize}
    \item \textbf{Span Detection}: Detecting all entity spans together without classification is a simpler problem for the model than full NER and hence we get better performance compared to \texttt{BERT-QA} model.
    
    \item \textbf{Span Classification}: Given that spans are detected correctly, this second step of the pipeline is relatively simple for the BERT model. On all datasets, we see above 90\% Micro-F1 on test set.
    
    \item \textbf{Pipeline}: The pipelined procedure gives comparable results to standard question answering based NER model. The main bottleneck lies in the span detection part. Since this procedure is pipelined, errors in first step propagate to the next step leading to an overall reduced performance.
\end{itemize}

\section{Learning Objective Variations}
An ML algorithm learns by optimizing its learning objective. For named entity recognition in supervised learning setup, we get a corpus labeled with gold entity mentions. As highlighted in section \ref{sec:nature_of_entities}, there may be an inherent labeling bias in the dataset. Some entities have more representation, more labeled mentions (\textit{high-resource}) while others are rare entities (\textit{low-resource}). With fewer samples, it becomes difficult for the model to learn good differentiating rules for extracting the low-resource entities. To handle this bias in dataset, it is common to give more importance/weight to low-resource entity samples in calculating the learning objective and optimizing on it. In this direction, we experiment with the variants detailed below.

\begin{itemize}
    \item \textbf{Cross-Entropy (CE) Loss}: This is the standard objective function in which we calculate the cross entropy between the predicted and gold labels for each token in the sentence.
    
    \item \textbf{Weighted Cross-Entropy Loss}: Here, based on the gold-labeled tag for a token we assign a weight to its contribution. All tokens which belong to a valid entity span contribute equally to the loss and all other tokens contribute with a reduced weight of 0.5. This helps because a large part of the corpus consists of tokens which belong to the \texttt{O} tag category. For instance, the \texttt{BioNLP13CG} corpus has 76.5\% tokens with \texttt{O} tag.
    
    \item \textbf{Punctuation Weighted CE Loss}: From qualitative analysis of misclassified samples in standard CE loss setup we noticed that the model is not able to learn good representations for special symbols like parenthesis, hyphen, period, slash etc. Hence, we force the model to learn these symbols well by penalizing the model twice if the misclassified token is a punctuation/special symbol.
    
    \item \textbf{Dice Loss}: As proposed by \cite{li2019dice}, we address the above mentioned data imbalance issue between entity and non-entity tokens using dice loss, based on S{\o}rensen-Dice coefficient\cite{} or Tversky index\cite{}. This helps because standard CE loss in accuracy-oriented while during evaluation we calculate F1-measure. The dice loss gives equal importance to false-positives and false-negatives at training time and hence reduces the discrepancy among these training and test time metrics.
    
    \item \textbf{CRF}: Apart from the cross-entropy objective, we also experiment by adding a CRF layer on the fully connected layer output in standard BERT-based model. CRF layer is found to work well with bidirectional LSTM\cite{ma2016end} in modeling output tag transitions and emissions.
    
\end{itemize}

\subsection{Experiment Details}
For CRF implementation, we use \texttt{torchcrf} python package.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
	\textbf{} & \textbf{BIONLP13CG} & \textbf{CoNLL 2003}\\\hline
	\texttt{CE Loss} & 85.99 & 91.36\\\hline
	\texttt{Weighted CE Loss} & 85.93 & todo\\\hline
	\texttt{Punctuation CE Loss} & 86.12 & todo\\\hline
	\texttt{Dice Loss} & 86.35 & 90.76\\\hline
	\texttt{CRF} & 86.20 & todo\\\hline
	\end{tabular}
    \caption{Results: Learning Objectives (Test set Micro-F1 in \%)}
    \label{tab:res_loss}
\end{table}

\subsection{Observations}
\begin{itemize}
    \item \texttt{Weighted CE Loss} is found to perform comparable to \texttt{CE Loss} and both do not perform as well as the other variants.
    
    \item In \texttt{BioNLP13CG} corpus, \texttt{Dice Loss} performs well as this corpus as several high and low resource entities (data imbalance). \texttt{Dice Loss} is not able to give its advantages with \texttt{CoNLL 2003} corpus since here all 4 entity types have a comparable and high representation.
    
    \item \texttt{Punctuation CE Loss} is able to make the model learn slightly better semantics of special symbols and hence performs better than the standard \texttt{CE Loss} counterpart.
    
    \item \texttt{CRF} is able to capture the output tag transitions better and hence performs better than standard \texttt{CE Loss} setting.
\end{itemize}

\section{Capturing Additional Token Semantics}
\label{sec:additional_token_semantics}

Both in \texttt{CNN-LSTM-CRF} and transformer-based architectures, we rely on on a pretrained BERT model. For biomedical datasets we use BioBERT which is trained on scientific and biomedical literatures while for English news datasets, we work with \texttt{bert-base-uncased} model which is trained on Wikipedia and books. BERT models rely on Word-Piece tokenizer\cite{} which considers breaking words into sub-words for representation. Even then from qualitative analysis we find that there are several terms whose semantics are not captured well by the existing BERT model. This causes errors which can be categorized as:

\begin{itemize}
    \item \textbf{Out of Vocabulary Tokens}: News articles and scientific texts both sometimes make use of abbreviations or localized entities which may be specific to that news article, event time or research paper, but are rare otherwise. Additionally biomedical texts also consist of chemical names in scientific form which includes numerals etc. which need to be extracted. Semantics of such terms is not captured well by generically pretrained models. Table \ref{tab:oov_issue} shows some examples from \texttt{BioNLP13CG} corpus.
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}\hline
    	\textbf{Entity} & \textbf{Misclassification Examples}\\\hline
    	\texttt{Gene\_or\_Gene\_Product} & DPD, Xhol, mutCK1delta, FAS\\\hline
    	\texttt{Simple\_Chemical} & MnCl2, AglRhz, NO\\\hline
    	\texttt{Cell} & LoVo, DeltaG45, BMSVTs\\\hline
    	\texttt{Amino\_Acid} & phosphoS727, Y705F\\\hline
    	\end{tabular}
        \caption{Out-of-Vocabulary tokens in \texttt{BioNLP13CG} corpus}
        \label{tab:oov_issue}
    \end{table}
    
    \item \textbf{Special Symbols}: Several entities to be extracted have hyphens, periods, parenthesis within them which are not captured well by pretrained BERT model leading to partial entity detection. Table \ref{tab:boundary_issue} gives some examples from \texttt{BioNLP13CG} corpus.
    
    \item \textbf{Modifier Suffix/Prefix}: Apart from the root entity required to be extracted the gold labels sometimes expect a modifier term as well which occurs as a prefix/suffix. Missing these again leads to boundary detection issues. Table \ref{tab:boundary_issue} gives some examples from \texttt{BioNLP13CG} corpus.
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}\hline
    	\textbf{Misclassification Category} & \textbf{Gold} & \textbf{Predicted}\\\hline
    	\texttt{Special Symbols} & L . Se ( + ) cells & L . Se\\\hline
    	\texttt{Special Symbols} & Gs - IB ( 4 - ) ion & Gs - IB ( 4\\\hline
    	\texttt{Modifier Suffix/Prefix} & epicardial coronary artery & coronary artery\\\hline
    	\texttt{Modifier Suffix/Prefix} & T140 analogs & T140\\\hline
    	\end{tabular}
        \caption{Boundary detection issues in \texttt{BioNLP13CG} corpus}
        \label{tab:boundary_issue}
    \end{table}
\end{itemize}

\subsection{Experiment Details}

To address the above mentioned issues, we provide additional inputs and infrastructure to the model to learn the underlying semantics better. In all these cases, we develop on top of the sequence labeling setup for NER.

\begin{itemize}
    \item \textbf{Special Symbol Features}: Before feeding the output of BERT model to final fully-connected classification layer, we add an extra one-hot dimension which is set if the current input token is a pure special symbol. This means we assign \texttt{1} for \textit{hyphen}(\texttt{-}), \textit{parenthesis}(\texttt{(} and \texttt{)}), \textit{comma}(\texttt{,}) etc. while assign \texttt{0} for tokens like \texttt{carbon}, \texttt{123}, or mixed format tokens like \texttt{Ca(2+)}, \texttt{AB-3} etc.
    
    \item \textbf{Word Type Features}: As an extension of the above special symbol features, here we associate each token with a word type as shown in Table \ref{tab:word_type_encoding} which is converted into a one-hot vector concatenated with BERT output embeddings before sending to final classifier layer.
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}\hline
    	\textbf{Word Type} & \textbf{Encoding}\\\hline
    	\texttt{[CLS] token} & 0\\\hline
    	\texttt{[SEP] token} & 1\\\hline
    	\texttt{all lowercase} & 2\\\hline
    	\texttt{all caps} & 3\\\hline
    	\texttt{first letter caps, rest lowercase} & 4\\\hline
    	\texttt{all digits} & 5\\\hline
    	\texttt{all special symbols} & 6\\\hline
    	\texttt{alphabets + digits} & 7\\\hline
    	\texttt{all the rest} & 8\\\hline
    	\end{tabular}
        \caption{Word Type Encoding}
        \label{tab:word_type_encoding}
    \end{table}
    
    \item \textbf{Character and Pattern Features}: Chemical formulas and scientific names generally follow a nomenclature convention or pattern. Similarly, out-of-vocabulary tokens also may have some intrinsic character level information which is not well captured by the BERT model which considers sub-words. Infact this issue with BERT is studied well in \cite{boukkouri2020characterbert} which propose to use a character CNN instead of word-piece tokenizer at the stating stage. Motivated by \texttt{CNN-LSTM-CRF} setup and this study, we do the following:
    
    \textbf{Modeling characters}. Each word is passed to BERT and in parallel, to five 1-dimensional CNNs with kernel sizes of 1 to 5, each having 16 input and 16 output channels. Input character is indexed and embedded into 16-dimensions through an embedding layer. The CNN outputs are concatenated and passed through a linear layer to get overall 768-dimensional output vector for each token.
    
    \textbf{Modeling patterns}. Each word is converted to a pattern (like regular expression, a denser space than simply all characters) converting all uppercase letters to \texttt{U}, lowecase to \texttt{L}, digits to \texttt{D} etc. and then sent to a separate character CNN (like the one described above) and then to a bidirectional LSTM to get contextual token embeddings.
    
    Finally, these character and pattern embeddings are concatenated with BERT outputs and fed to final classifier layer for tag classification.
    
    \item \textbf{Part-of-Speech and Dependency Parse Features}: Concatenate BERT embeddings with the one-hot POS and one-hot dependency parse features before feeding to final classifier layer.
    
    \item \textbf{Head Tokens}: BERT uses word-piece tokenizer and hence can break a given single token into multiple sub-words. Instead of doing a token classification on each of these sub-words and making sure everything is correct, it is simpler to take the embedding output for the first sub-word (\textit{head}) for each token. This technique is also used in the original BERT paper \cite{devlin2018bert} for their NER experiments. 
    
    \item \textbf{Highway Network}: Instead of using a simple concatenation of character features with BERT as done in previous experiment, here we create a highway network\cite{}, similar to the one used in BiDAF\cite{} architecture in which we train a gated network to learn when to use BERT vectors and when to use the additional character-level information.
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG} & \textbf{CoNLL 2003}\\\hline
	\texttt{Vanilla BERT} & 85.99 & 91.36\\\hline
	\texttt{Special Symbol} & 86.63 & 91.67\\\hline
	\texttt{Word Type} & 86.50 & 91.55\\\hline
	\texttt{Character/Patterns} & 86.44 & 91.08\\\hline
	\texttt{Part-Of-Speech} & 86.11 & todo\\\hline
	\texttt{Dependency Parse} & 86.20 & todo\\\hline
	\texttt{Head Tokens} & 86.17 & 91.49\\\hline
	\texttt{Special Symbol + Head Tokens} & 86.36 & 91.68\\\hline
	\texttt{Highway Net} & todo & todo\\\hline
	\end{tabular}
    \caption{Results: Token Semantics (Test set Micro-F1 in \%)}
    \label{tab:res_token_semantics}
\end{table}

\subsection{Observations}
Our results are summarized in Table \ref{tab:res_token_semantics}. We also experimented with combinations of the above described features together but omit the results from this report if not found to be significant. We make the following observations:

\begin{itemize}
    \item All the proposed additional features are found to improve upon the \texttt{Vanilla BERT} model on \texttt{BIONLP13CG} dataset. On \texttt{CoNLL 2003} dataset, some features are found to be more effective than others.
    
    \item Handling special symbols is a primary issue of \texttt{Vanilla BERT}. Explicitly handling it is found to be most helpful among all the other features across both datasets and on both models \texttt{Special Symbol} and \texttt{Special Symbol + Head Tokens}.
    
    \item Extending the special symbol features to word types may be giving slightly mixed signals to the model. It hence gives the second best improvements over the vanilla version on both datasets.
    
    \item \texttt{Character/Pattern} modeling helps in the biomedical text setting since it helps understand semantics of some chemical names etc. However, giving importance to intrinsic patterns seems to send slightly conflicting signals on general English text and hence gives lower performance on \texttt{CoNLL 2003} data.
    
    \item Part of Speech and Dependency Parse features do give some additional insights to the model over vanilla BERT and hence give better performance. Dependency parse features capture entity inter-dependencies which helps more than just the part-of-speech of tokens.
    
    \item Considering only head tokens again helps over vanilla modeling on both datasets. When combined with special symbol features it helps give the highest result on \texttt{CoNLL 2003} data. On \texttt{BIONLP13CG}, the effect is reduced as compared to \texttt{Special Symbol}. We suspect that this is because of some loss of intrinsic sub-word details when considering only the head tokens, which can be crucial for biomedical and scientific texts since they have lots of out-of-vocabulary chemical/gene names etc.
    
    \item Highway network is found to be more effective than plain concatenation of features with BERT outputs on both datasets, since it gives the model more flexibility to prioritize among different sources of information dynamically on a case-by-case basis.
\end{itemize}

\section{Training Effectiveness Study}
In the previous section \ref{sec:additional_token_semantics}, we mostly try to give additional information to a BERT architecture by concatenating BERT outputs with some additional feature vectors before final classification. Giving additional information to a model is one thing but whether the model is actually able to pick cues from the additional information or not, is another. In this section, we study the training effectiveness on \texttt{BioNLP13CG} dataset in \texttt{Sequence Labeling} setting using \texttt{BioBERT-Base} model.

\subsection{Feeding Answer as Input}

To study the training effectiveness, we give the model the best ideal-case information, i.e. for each token, we feed its gold label as a one-hot vector. This is concatenated with BERT output before feeding to final classifier. We study the following variants:

\begin{itemize}
    \item \texttt{BERT(Freeze) + Answer}: We freeze the BERT model parameters and concatenate the answer vectors. This effectively reduces the no. of trainable parameters to a few thousand. We try this in two settings, low learning rate of $10^{-5}$ (recommended BioBERT learning rate\footnote{https://github.com/dmis-lab/biobert\#named-entity-recognition-ner}) and high learning rate of $0.005$.
    
    \item \texttt{BERT + Answer}: This mimics the real-world scenario where the BERT model is fine-tuned with given additional information at a low learning rate of $10^{-5}$.
    
    \item For comparison with the above, we also present the results for simple \texttt{BERT} (fine-tuned) and \texttt{BERT(Freeze)} models as well.
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}\hline
	\textbf{Model} & \textbf{Learning Rate} & \textbf{BIONLP13CG}\\\hline
	\texttt{BERT} & $10^{-5}$ & 85.99\\\hline
	\texttt{BERT + Answer} & $10^{-5}$ & 86.35\\\hline
	\texttt{BERT(Freeze)} & $0.005$ & 75.42\\\hline
	\texttt{BERT(Freeze) + Answer} & $10^{-5}$ & 63.89\\\hline
	\texttt{BERT(Freeze) + Answer} & $0.005$ & 100.00\\\hline
	\end{tabular}
    \caption{Results: Training Effectiveness - Feed Answer as Input (Test set Micro-F1 in \%)}
    \label{tab:res_training_ans_input}
\end{table}

From the results summarized in Table \ref{tab:res_training_ans_input}, we observe that with a very low learning rate of $10^{-5}$, the model is not able to learn from the additional answer information provided (both with frozen BERT and otherwise). At a higher learning rate of $0.005$, the model catches the provided cues well.

\subsection{What happens at high learning rate?}

From the previous section, we see that we need a relatively higher learning rate to learn from additional information fed to the model. But with BERT fine-tuning, it is recommended to use a very low learning rate in the order of $10^{-5}$. To study this, we pass answer spans as input to the model. Basically, for each token, we add a one-hot dimension which is set if the token belongs to a valid entity else it is set to $0$. This means the model is actually being fed the gold spans and all it has to do is span classification. This should be relatively easy and give good results as seen previously in Section \ref{sec:span_pipeline}. We experiment with the following variants:

\begin{itemize}
    \item \texttt{BERT (Freeze)}: Freezing BERT model and passing through a single trainable classifier layer. This model has only around $26,000$ trainable parameters. 
    
    \item \texttt{BERT (Freeze) + Gold Span}: Same as the above but with gold spans given as additional input. So, this model is expected to perform better than \texttt{BERT (Freeze)}.
    
    \item \texttt{BERT + Gold Span}: Same as the above model but with trainable BERT model. We try training this in two different settings, with low learning rate of $10^{-5}$ and high learning rate of $0.005$.
    
    \item \texttt{BERT}: This is the standard BERT model with final classification layer and no additional feature inputs fine-tuned at learning rate of $10^{-5}$.
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}\hline
	\textbf{Model} & \textbf{Learning Rate} & \textbf{BIONLP13CG}\\\hline
	\texttt{BERT(Freeze)} & $0.005$ & 75.42\\\hline
	\texttt{BERT} & $10^{-5}$ & 85.99\\\hline
	\texttt{BERT(Freeze) + Gold Span} & $0.005$ & 79.18\\\hline
	\texttt{BERT + Gold Span} & $10^{-5}$ & 85.78\\\hline
	\texttt{BERT + Gold Span} & $0.005$ & 0.0\\\hline
	\end{tabular}
    \caption{Results: Training Effectiveness - Feed Gold Span as Input (Test set Micro-F1 in \%)}
    \label{tab:res_training_span_input}
\end{table}

From Table \ref{tab:res_training_span_input}, we make the following observations:

\begin{itemize}
    \item From \texttt{BERT (Freeze)} and \texttt{BERT (Freeze) + Gold Span}, we observe that indeed giving gold span information helps the model.
    
    \item From \texttt{BERT(Freeze) + Gold Span} and \texttt{BERT + Gold Span (LR: $10^{-5}$)}, we observe that indeed fine-tuning is able to learn the semantics of entities much better than when using the BERT embeddings right out-of-the-box.
    
    \item However, from \texttt{BERT} and \texttt{BERT + Gold Span (LR: $10^{-5}$)}, we observe that with a low learning rate, the model is not able to focus on and effectively utilize the gold span information.
    
    \item Finally, from \texttt{BERT + Gold Span (LR: $10^{-5}$)} and \texttt{BERT + Gold Span (LR: $0.005$)}, we observe that increasing the learning rate has a deteriorating effect on the pretrained BERT parameters and the rigorous push from high learning rate pushes the model to an unsatisfactory local optima.
\end{itemize}

% \section{Tagging Scheme Variation}
% As described in Section \ref{sec:tagging_scheme}, in this section, we study how much impact do different output tagging schemes have on boundary detection and learning of the model. We test the \texttt{2-Tag}, \texttt{BIO} and \texttt{BIOE} tagging schemes in question answering setup on \texttt{BIONLP13CG} corpus.

% We omit the experiments with sequence labeling setup since for \texttt{K} output tags, \texttt{2-Tag} scheme gives \texttt{K + 1} output tags (\texttt{+1} for \texttt{O} tag). With \texttt{BIO} scheme, we have \texttt{2K + 1} output tags and with \texttt{BIOE} scheme, we have \texttt{3K + 1} tags. For \texttt{BIONLP13CG} corpus which already has \texttt{K = 16}, the \texttt{BIOE} scheme makes number of output tags as \texttt{49}, which is too large to train well since we don't have enough training data. However, for the question answering setup, the number of output tags remains \texttt{3} for \texttt{2-Tag}, \texttt{4} for \texttt{BIO} and \texttt{5} for \texttt{BIOE} scheme which is manageable.

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|}\hline
% 	\textbf{Model} & \textbf{BIONLP13CG}\\\hline
% 	\texttt{BERT-QA (2-Tag)} & todo\\\hline
% 	\texttt{BERT-QA (BIO)} & 86.15\\\hline
% 	\texttt{BERT-QA (BIOE)} & 86.45\\\hline
% 	\end{tabular}
%     \caption{Results: Tagging Scheme Variation in QA Setup (Test set Micro-F1 in \%)}
%     \label{tab:res_tagging_scheme_qa}
% \end{table}

% From results in Table \ref{tab:res_tagging_scheme_qa}, we observe that as expected, explicitly modeling the begin and end boundaries performs the best while not modeling start and end at all, in \texttt{2-Tag} scheme performs the least among them. However this will have lesser number of parameters to train and more representative samples for each case.

\section{Pretrained Model Variation}
In almost all of our model variations discussed we use a pretrained BERT model and either fine-tune it or freeze its parameters and use it out-of-the-box. Even though the underlying model architecture remains that of a transformer encoder, based on which dataset the pretrained model is trained on and what objective function is used, there are several variants. We study the effect of this pretraining for NER in sequence tagging setup through the following variants. Note that in all our experiments we work with the BERT-Base architecture which consists of around 110M parameters.

\begin{itemize}
    \item \texttt{BERT-Base-Uncased}: Proposed by \cite{devlin2018bert}, this model is trained on English text from Wikipedia and BookCorpus\cite{moviebook} which totals around 16GB of uncompressed text. The model is uncased. It is trained on masked language modeling (MLM) and next sentence prediction objective. We use \texttt{bert-base-uncased} model provided by HuggingFace\footnote{https://huggingface.co/bert-base-uncased} for our \texttt{CoNLL 2003} and \texttt{OntoNotes 5.0} corpora.
    
    \item \texttt{RoBERTa-Base}: Proposed by \cite{}, this model is trained on English text from 5 different datasets totalling around 160 GB of uncompressed text. The model is cased and trained on only the masked language modeling (MLM) objective. We use \texttt{roberta-base} model provided by HuggingFace\footnote{https://huggingface.co/roberta-base} for our \texttt{CoNLL 2003} and \texttt{OntoNotes 5.0} corpora.
    
    \item \texttt{BioBERT-Base}: Proposed by \cite{}, this model is trained on English biomedical literature including PubMed abstracts and PMC full text articles. The model is cased and trained on same MLM and next sentence prediction objectives proposed in standard BERT model. We use \texttt{BioBERT-Base v1.1} model provided on GitHub\footnote{https://github.com/dmis-lab/biobert\#download} and import it in HuggingFace as \texttt{dmis-lab/biobert-base-cased-v1.1}. We use this model on our biomedical corpora like \texttt{BIONLP13CG} and \texttt{JNLPBA}, since models pretrained on biomedical and scientific texts are found to capture semantics more effectively than those trained on general English text.
    
    \item \texttt{SciBERT-Base-Uncased}: Proposed by \cite{}, this model is trained on full texts of papers on Semantic Scholar\footnote{https://www.semanticscholar.org/}. The model is uncased and trained using the MLM and next sentence prediction objectives originally proposed by the BERT paper. However, they are trained using \texttt{SciVocab}, a specially created WordPiece vocabulary for scientific texts. Just like BioBERT, we use this for \texttt{BIONLP13CG} and \texttt{JNLPBA} datasets.
    
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG} & \textbf{JNLPBA}\\\hline
	\texttt{BERT-Base-Uncased} & 82.63 & -\\\hline
	\texttt{BioBERT-Base} & 85.99 & 74.35\\\hline
	\texttt{SciBERT-Base-Uncased} & 86.01 & todo\\\hline
	\end{tabular}
    \caption{Results: Pretrained Model Variation (Test set Micro-F1 in \%)}
    \label{tab:res_pretrained_model_bio}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}\hline
	\textbf{Model} & \textbf{CoNLL 2003} & \textbf{OntoNotes 5.0}\\\hline
	\texttt{BERT-Base-Uncased} & 91.36 & 74.35\\\hline
	\texttt{RoBERTa-Base} & 91.19 & todo\\\hline
	\end{tabular}
    \caption{Results: Pretrained Model Variation (Test set Micro-F1 in \%)}
    \label{tab:res_pretrained_model_general}
\end{table}

In Tables \ref{tab:res_pretrained_model_bio} and \ref{tab:res_pretrained_model_general}, we present our results. We observe that:

\begin{itemize}
    \item Performance of \texttt{BERT-Base-Uncased} model and \texttt{RoBERTa-Base} models on \texttt{CoNLL 2003} and \texttt{OntoNotes 5.0} corpora is almost comparable with \texttt{BERT-Base-Uncased} having a slight edge.
    
    \item On biomedical corpora, \texttt{SciBERT-Base-Uncased} is found to perform better than \texttt{BioBERT-Base v1.1} model. As expected, both of these are better than using \texttt{BERT-Base-Uncased} in the biomedical setting. Hence, we omit the \texttt{BERT-Base-Uncased} experiment with \texttt{JNLPBA}.
\end{itemize}

\section{Clustering and Segregation of Diverse Entities}
\label{sec:clustering}
Here, we focus on \texttt{BIONLP13CG} dataset. Looking at the entity distribution in Table \ref{tab:bio_entity_distribution}, we see that the dataset has lots of samples of \texttt{Gene\_or\_gene\_product} along with \texttt{Cancer} and \texttt{Simple\_chemical}. In the question answering setup as we studied in Section \ref{sec:question_answering}, the entity name semantics is not too essential and the model still learns to differentiate their mentions accurately. 

Here, we develop on that idea and try to break a diverse entity group \texttt{Gene\_or\_gene\_product} into sub-groups (or, sub-entities) and train a model to learn each of those sub-entities and differentiate them from each other. The idea is that dividing a diverse group into sub-entities will help reduce the diversity of a big entity group and make it simpler for the model to differentiate cases/variations. At inference time, the sub-entities are remapped to the parent entity and F1-score is calculated.

\subsection{Experiment Details}
We work with question answering setup with only 3 entities, \texttt{Gene\_or\_gene\_product}, \texttt{Cancer} and \texttt{Simple\_chemical}. All other entities are ignored for the experiment. The procedure is described below:

\begin{itemize}
    \item Collect all entity mentions of \texttt{Gene\_of\_gene\_product}. Pass their corresponding sentences to \texttt{BioBERT-Base-Uncased} model and concatenate the output embeddings of first and last sub-word labeled as \texttt{Gene\_of\_gene\_product}. This is the embedding of that mention.
    
    \item If there are multiple instances of same mention, then take mean of their mention embeddings across sentences.
    
    \item Reduce mention embeddings to 100-dimensional vectors using principal component analysis (PCA). Next take their tSNE\cite{} projections to convert each mention to a 2-dimensional vector representation.
    
    \item \textbf{Clustering}: Fix number of clusters to 4 (hyper-parameter). Randomly select 4 mention instances as centers and apply K-Medoids clustering with euclidean distance among tSNE representations as the distance metric. Note that we worked with cosine distance and KL-Divergence as well on tSNE representations as well as on 100-dimensional PCA representation. However this presented setup is found to work the best.
    
    \item After clustering, we relabel the corpus tagging each mention to its cluster. Each cluster is considered a sub-entity of \texttt{Gene\_of\_gene\_product}. The entities are named from \texttt{Gene\_of\_gene\_product0} to \texttt{Gene\_of\_gene\_product3}.
    
    \item Next we train a simple question answering NER model with this new labeled dataset. 
    
    \item During inference time, the model outputs each mention into its sub-entity. We post-process the labels and map each sub-entity output to \texttt{Gene\_of\_gene\_product} and then calculate Micro-F1.
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG}\\\hline
	\texttt{BERT-QA} & 87.97\\\hline
	\texttt{BERT-QA (With Clustering)} & 86.66\\\hline
	\end{tabular}
    \caption{Results: Clustering/Segregation of Diverse Entities (Test set Micro-F1 in \%)}
    \label{tab:res_clustering}
\end{table}

\subsection{Observations}
From results in Table \ref{tab:res_clustering}, we observe that this sub-entity extraction technique gives comparable performance to the original setup but is not able to give any improvements. This can be attributed to the fact that our clustering has been on the basis of semantics captured by out-of-the-box pretrained BioBERT-Base model which in the first place may not be able to capture the semantics of many entities well. In addition, the criteria for clustering is semantic similarity captured by the embeddings and not word-pattern similarity. In the current clusters, there is still lots of diversity which the model has to capture. Hence, we conclude that the potential of this technique is only limited by the quality of clustering we are able to achieve. With better word-pattern oriented clusters, this technique is bound to get much better results.

\section{Handling Nested Entities}
% Such examples can be found in biomedical domain in GENIA corpus\cite{} and news article domain in ACE-2004\cite{} and ACE-2005\cite{} corpora. \cite{finkel2009nested} align sentences into a tree-structure with entities for handling this and \cite{li2019unified} extract each entity separately by querying a sentence multiple times in a question-answering framework. To make data annotation even more complicated, Katiyar and Cardie [122] reported that nested entities are fairly common: 17% of the entities in the GENIA corpus are embedded within another entity; in the ACE corpora, 30% of sentences contain nested entities. Our goal is to develop an approach for named entity recognition that is domain-agnostic and which does not develop biases towards commonly seen entities and learns a meaningful representation for low-resource entities as well. Hence, we conduct our experiments on multiple datasets as detailed below. 

As described in \ref{sec:nature_of_entities}, not always are entities of interest completely disjoint of each other in a sentence. There may be nested and overlapping cases. In biomedical and scientific texts, it is a common trend to find chemical names and genes in a nested structure, for example, \textit{tyrosine} is a \texttt{Simple\_chemical} while \textit{tyrosine kinase} is a \texttt{Gene\_or\_gene\_product}. \cite{wang2018penner} work on nested entity extraction by automatically constructing meta-patterns. Similarly, in general English text, \textit{Goldman Sachs, Bangalore} is an \texttt{Organization} while \textit{Bangalore} is a \texttt{Location}. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG}\\\hline
	\texttt{Flat Entities} & 86.45\\\hline
	\texttt{Flat + Nested Entities} & 85.90\\\hline
	\end{tabular}
    \caption{Results: Nested Entities (Test set Micro-F1 in \%)}
    \label{tab:res_nesting}
\end{table}

\subsection{Experiment Details and Observations}

In \texttt{BIONLP13CG} corpus, there are around 1\% nested entity structures. It is not a very significant proportion but still helps us understand how well our model can handle such nested cases. The sequence tagging setup just outputs tags for each token sequentially and hence cannot output multiple tags for the same token. However, in the question answering setup, for each sentence, we ask multiple separate questions enquiring for different entities. Hence, this setup can handle nested entities. For the example mentioned above, we can once ask for \texttt{Organization} in text and get \textit{Goldman Sachs, Bangalore} and then ask for \texttt{Location} and get \textit{Bangalore}. From the results in Table \ref{tab:res_nesting}, we observe that our model is able to perform comparably well on flat as well as nested entity extraction.

\section{Multi-Sentence Context}
In most datasets we use like \texttt{BIONLP13CG} and \texttt{CoNLL 2003}, we get individual input sentences at training/test time. However, in real-world scenarios, in information extraction domain, we generally have a paragraph or long text from which we have to extract entities. So, connected sentences can be expected. In the individual sentences of the benchmark datasets, we may expect to find some co-reference resolution problems or unknown terms which were probably defined in a previous sentence that the model currently does not know of. This could lead to sub-optimal performance. 

Even in the benchmark datasets like \texttt{BIONLP13CG} and \texttt{CoNLL 2003}, the context sentences are present but not being used. \texttt{BIONLP13CG} dataset consists of abstracts from biomedical research papers. On each of these abstracts individual sentences have been segmented out to create the train/dev/test sets. In \texttt{CoNLL 2003} dataset, there is a special \texttt{--DOCSTART--} tag which signifies the end of a full news article and the start of another. Hence, again by looking at the previous and next sentences we can get the context for current training/text sample. \cite{luoma2020exploring} explore this concept of additional context and develop a majority voting scheme to understand context sentence weightage to achieve better performance on \texttt{CoNLL 2003} NER dataset. We explore the effect of context in the biomedical domain.

\subsection{Experiment Details}
To study the effect of additional context provided to the model, we parse the \texttt{BIONLP13CG} dataset again and create training samples consists of around 300 tokens. When broken down using WordPieceTokenizer by BERT, this fits the maximum BERT window size of 512 sub-words. In these 300 token samples, we easily get around 3-4 sentences on an average. Hence, in terms of context, this dataset is much richer but yet 3-4 times shorter. For the last chunk which may have much lesser than 300 tokens, we retain it as it is, as a separate training sample.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}\hline
	\textbf{} & \textbf{BIONLP13CG} & \textbf{BIONLP13CG-Context}\\\hline
	\texttt{Train (\# Sentences)} & 3033 & 397\\\hline
	\texttt{Dev (\# Sentences)} & 1003 & 122\\\hline
	\texttt{Test (\# Sentences)} & 1906 & 249\\\hline
	\texttt{Avg. Sentence Length (\# Tokens)} & 27.6 & 223.9\\\hline
	\end{tabular}
    \caption{Multi-Sentence Context Dataset Stats}
    \label{tab:context_bio_dataset}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Model} & \textbf{BIONLP13CG}\\\hline
	\texttt{Normal Dataset} & 86.45\\\hline
	\texttt{Multi-Sentence Context Dataset} & 85.90\\\hline
	\end{tabular}
    \caption{Results: Multi-Sentence Context (Test set Micro-F1 in \%)}
    \label{tab:res_context_bio}
\end{table}

\subsection{Observations}
From the stats reported in Tables \ref{tab:context_bio_dataset} and \ref{tab:res_context_bio}, we observe that forming longer and more content rich training samples causes our overall dataset size to be significantly reduced. This is a cause of concern and hence additional context is not able to give performance boost to the existing standard training setup.

\section{Comparative Precision/Recall Analysis}
\label{sec:precision_recall_analysis}
Next, we do a quantitative comparison among the several variations studied in the previous sections and look at their micro-averaged precision, recall and F1-scores over multiple datasets.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\hline
     & \multicolumn{3}{c|}{\textbf{BIONLP13CG}} & \multicolumn{3}{c|}{\textbf{CoNLL 2003}} & \multicolumn{3}{c|}{\textbf{JNLPBA}}\\\hline
	 & P & R & F1 & P & R & F1 & P & R & F1\\\hline
	\texttt{BERT} & 86.17 & 85.82 & 85.99 & 91.24 & 91.48 & 91.36 & 70.97 & 78.07 & 74.35\\\hline
	\texttt{Dice Loss} & 86.68 & \textbf{86.03} & 86.35 & 91.05 & 90.47 & 90.76 & \textbf{72.05} & 78.23 & \textbf{75.01}\\\hline
	\texttt{Span-Based} & 86.33 & 85.47 & 85.89 & 91.46 & \textbf{91.82} & 91.64 & 71.14 & \textbf{79.34} & \textbf{75.01}\\\hline
	\texttt{QA} & 88.62 & 84.39 & 86.45 & 91.54 & 90.80 & 91.17 & 71.97 & 78.11 & 74.92\\\hline
	\texttt{QA(Where)} & \textbf{89.21} & 84.58 & \textbf{86.83} & \textbf{92.47} & 91.19 & \textbf{91.82} & 71.45 & 78.11 & 74.64\\\hline
	\texttt{Punctuation} & 87.62 & 85.66 & 86.63 & 91.75 & 91.60 & 91.67 & 70.42 & 78.47 & 74.23\\\hline
	\end{tabular}
    \caption{Results: Precision/Recall Comparison}
    \label{tab:res_precision_recall}
\end{table}

\section{State-of-the-art Comparison}
\label{sec:sota_comparison}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}\hline
	\textbf{} & \textbf{BIONLP13CG} & \textbf{CoNLL 2003} & \textbf{JNLPBA}\\\hline
	\texttt{Span-Based} & 86.83 & 91.82 & 75.01\\\hline
	\texttt{SOTA} & 85.56* & 94.3* & 81.29*\\\hline
	\end{tabular}
    \caption{Results: Comparison with SOTA (Test set Micro-F1 in \%)}
    \label{tab:context_bio_dataset}
\end{table}