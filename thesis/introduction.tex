% why information extraction?
With the onset of the digital age in the past few decades, the amount of data has been increasing almost exponentially. This makes it increasingly difficult to manually read everything and extract important information. Hence, automated information extraction becomes an important problem and has been a popular area of research in the natural language processing community. 

% applications of IE (industry documents)
Most modern-day organizations have to deal with lots of documents. These could be invoices from purchases, employee salary slips, contracts, resumes, communications with clients, emails, filings and reports, discussion notes etc. This makes a vast and diverse source of content-rich data. From an information extraction perspective, this provides a gold mine to extract relevant information which can help a company take informed decisions and make their business model better. At the same time, some sources of information are private and sensitive. This includes personal employee records, appraisal statements, their identity information like social security number (in US) etc. This private information is protected by laws like CCPA (California Consumer Privacy Act) etc. Hence, from an information security perspective, we need systems that can automatically identify sensitive content in documents, like, their name, phone number, address, identity information etc. Such documents should be protected or the sensitive content must be redacted to avoid information leakage.

% applications of IE (biomedical)
With the blooming research in science and technology, the amount of research papers, journals and studies published as been ever increasing. For any researcher to follow all papers even in his/her own field of study has become difficult. As an example, we can consider the COVID-19 pandemic. During the time, research and experiments have been conducted round the clock and white papers with findings have been published. To device a cure, identify the proper medication and develop vaccines, having a computer automatically filter out the important content from documents (including chemical names, pathogens, diseases/medical conditions, diagnosis, effects) surely facilitates and drives new research directions. It also helps uncover latent trends. For example, a new virus strain might have characteristics very much similar to a previously studied pathogen and hence similar line of treatment might be effective against it. Automated extraction saves human efforts and time taken for such tasks.

% applications of IE (web/blogs/stock market)
In this internet saga, amount of content found on websites is exploding. There are blog posts, online news articles etc. In the e-commerce industry there are customer reviews of products on websites like Amazon. All these vast volumes of raw data have lots of valuable information embedded within them. From an e-commerce perspective, users are interested in knowing how their peers found a particular product, or some aspect/feature of the product, like the camera quality of IPhone 12. From news articles, blogs and reports, one can figure out how a particular sector/industry is performing and hence predict if investing in the industry is going to do any good. The solution to all these questions lies again in automated information extraction. 

% IE (text, audio, images)
All this motivates the position of information extraction (IE) as an integral area of research and shows how deep an impact it has on our modern-day life. And till now, we have only been focusing on information extraction from text. There is a whole lot of information one can capture from images and audio as well. As part of this thesis, however, we will focus on information extraction from vast volumes of unstructured text.

% IE sub-parts (NER, relation extraction, sentiment)
IE in itself is a vast area and can be broken down into sub-domains. Named entity recognition deals with extracting entities like \textit{person name}, \textit{organization name}, \textit{location}, \textit{phone number}, \textit{SSN} etc. from text. Relation extraction deals with extracting the relationship connecting entities in a sentence, for example, Linda \textit{works} for the Bank of America. Here, \textit{works} defines the relationship between \textit{Linda} and \textit{Bank of America}. Sentiment Analysis deals with extracting at a high level, the sentiment conveyed by a sentence. This may be crucial for consumer reviews on e-commerce websites. It can help analyse the market response for a product. In this study, we will be focusing on the named entity recognition task.

% low-resource vs high resource entities
Concretely, named entity recognition is a sequence tagging task, where given a sentence as a sequence of tokens, we classify each token into a entity class or if there is no entity in the token, then we tag it into a special \texttt{NONE} or \texttt{O} class. This problem can be modelled as a supervised learning task, where we get a bunch of labelled gold sentences through which we learn a model and then use this trained model to classify tags and identify entities in unseen sentences. The model is evaluated by checking the \textit{precision}, \textit{recall} and \textit{F1-score} against a gold-labelled test set. Traditionally, Hidden Markov Models and Maximum Entropy Models have been popular for this task.

With the recent advances in deep learning and language modelling in the last decade, we have recurrent neural networks, LSTMs and now Transformer models which help capture the long-term dependencies among words in a sentence and hence are able to extract entities well. However, these complex deep learning models require a large amount of labelled training data which means that we need a large number of labelled examples of each entity type. This, unfortunately, becomes a bottleneck. Some entities like \textit{person names}, \textit{organizations}, \textit{locations} etc. are popular and have lots of labelled examples. We say, these are \textit{high-resource} entities. But, some entities are very specific, like \textit{SSN}, \textit{bank account number}. They are neither very frequent in text nor they have large number of training examples. These are called \textit{low-resource} entities. In this thesis, we present some experiments to substantiate the hypothesis that the knowledge of known \textit{high-resource} entities can guide the extraction of \textit{low-resource} entities when the two occur in close vicinity of each other in a sentence. As an example, knowing that a sentence has an \textit{address} (high-resource entity) means there is a high likelihood of a \textit{phone-number} (low-resource entity) in its close context.

This thesis presents a detailed NER analysis across different domains like, English newspaper articles, bio-medical literature and cyber-security texts. We present a general domain-agnostic model architecture for NER and also present detailed ablations of the model components. Additionally, we do a thorough qualitative analysis of the used corpora and present the bottlenecks of existing benchmarks as well as concrete scopes of improvement to give direction to future research.