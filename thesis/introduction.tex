% Motivations for IE
As quoted by Forbes\footnote{https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/?sh=7cf99c3f60ba}, 2.5 quintillion bytes of new data is generated every day and this number is fast growing. In fact, 90 percent of all the data has been generated just in the last 2 years. Quite rightly said, it is indeed a \textit{data-driven world}, and the future is going to be no different. This vast multitude of raw data is a gold mine with an abundance of knowledge hidden inside it. A lot of it is in the form of unstructured text including books, research papers, news articles, blog posts, customer reviews, tweets etc. However, the pace of data growth has made it humanly impossible to manually browse through everything and get the required knowledge thus motivating research towards automated information extraction and knowledge discovery. 

In essence, the ultimate goal is to acquire knowledge from raw data which in-turn helps guide decision making. This can be viewed as a step-by-step process in which the  first step is to parse large volumes of raw text and extract informing entities along with their inter-relationships. Next is to organize this in the form of a knowledge graph preserving the interconnections. Finally, given a plain text query from the user, convert it into some graph operations to retrieve and return the relevant results.

In this thesis we focus on the first step i.e. information extraction (IE) which itself covers a diverse range of tasks. Named Entity Recognition (NER) deals with the extraction of important entities of interest from text. Relation extraction is the process of extracting inter-relationships among informing entities. Sentiment analysis deals with classifying the overall sentiment conveyed in given text. Question answering is the study of extracting an answer for a given question from a given input text. We primarily focus on the named entity recognition task here and next look at some of its applications.

% applications of IE (industry documents)
\textbf{Industry Documents}. Most modern-day organizations have to deal with lots of documents. These include annual reports, purchase invoices, salary slips, client contracts, resumes, emails etc. This makes up a vast and diverse pool of content-rich data which is persisted from a regulatory perspective and otherwise. Named entity recognition on this data can help draw important insights from past decisions and further improve the current business model. However, many such documents contain protected and sensitive information interspersed within. For example, employee records contain their address, date of birth, phone number, social security number (SSN) etc. which is private information protected by laws like CCPA(California Consumer Privacy Act) etc. Another application of NER, from an information security perspective, is to identify and redact such content and prevent sensitive information leakage.

% applications of IE (scientific literature)
\textbf{Scientific Literature}. The research community is ever flourishing, and it has become increasingly difficult for any researcher to remain up to date with all the latest developments even in their domain itself. As quoted in the first ever machine generated book of Chemistry published in Springer Nature\cite{writer2019lithium}, even in the niche research domain of Li-ion batteries, there were more than 53,000 research papers released in just the past 3 years. It is hence essential to have information extraction and summarization tools to filter out important content and NER becomes a vital step in this process.

% applications of IE (web data)
\textbf{Web Data}. In this age of the internet, the amount of online content has been exploding. There are around 500 million tweets everyday\footnote{https://www.internetlivestats.com/twitter-statistics/}. E-commerce has been blooming on websites like Amazon\footnote{https://www.amazon.com/}. According to some stats\footnote{https://landingcube.com/amazon-statistics/}, Amazon ships around 1.6 million packages per day. After making purchases, people voice their opinions and compare products and their features by writing reviews. Around 79\% of customers check posted reviews before making a purchase. These vast volumes of reviews have lots of valuable information embedded including product names, their features, opinion predicates etc. and is a trending NER application.

NER has been a popular research area for long and has seen transitions in parallel, with advances in the field of machine learning. Broadly, NER techniques can be categorized into rule-based, unsupervised, feature engineered and more recently, deep learning-based techniques. For NER, it is important to be able to understand the semantics of the sentence and how different entities are woven together by the rules of grammar and interacting with each other. In recent years, deep learning techniques like recurrent neural networks, LSTMs\cite{hochreiter1997long} and now transformer-based\cite{vaswani2017attention} architectures like BERT\cite{devlin2018bert} have made great progress in language modeling and capturing these inter-token dependencies which has aided the performance of named entity recognition. 

% However, there are still some open challenges to address:
% \begin{itemize}
%     \item Language dynamics drastically changes between general English news text and content-heavy scientific research literature. This also has impact on corresponding NER systems.
    
%     \item Deep learning approaches require a large amount of labeled training data. However, several entities requiring extraction do not have enough labeled samples making it difficult to extract them. (We handle this by pattern modeling)(We also hypothesize that information of high-resource entities facilitates the extraction of low-resource entities in its vicinity)
    
%     \item Different entities have different semantics which may require explicit modeling. Entities like \texttt{Person}, \texttt{Location} are wordy, while others like \texttt{Date}, \texttt{SSN}, \texttt{Telephone} can be numeric codes having patterns within them. (We handle this by parallel Char-CNN+Patterns)
% \end{itemize}

In this work, we use the popular \texttt{CNN-LSTM-CRF} and BERT models as baselines and develop on top of them. We present a thorough study and compare and contrast the NER task from three different perspectives, as a sequence labeling problem, a question answering-based approach and span detection and classification task. The contributions of our work are three-fold:

\begin{itemize}
    \item We introduce a novel pattern modeling approach which converts sparse character space to dense pattern space for more effective training of alphanumeric entities even with minimal training samples.
    
    \item Our system achieves new state-of-the-art on \texttt{BIONLP13CG} dataset which consists of 16 fine-grained alphanumeric named entity types and achieves competitive results in both biomedical as well as general English news domains.
    
    \item We show that mere concatenation of additional semantic features with BERT (transformer-based) features is not able to achieve its full modeling potential at the recommended low learning rates. More sophisticated feature fusion is essential.
\end{itemize}

% In this work, we focus on latest present a detailed NER analysis across different domains including news articles and biomedical literature. We present a general domain-agnostic model architecture for NER and also present detailed ablations of the model components. Additionally, we do a thorough qualitative analysis of the used corpora and present the bottlenecks of existing benchmarks as well as concrete scopes of improvement to give direction to future research.