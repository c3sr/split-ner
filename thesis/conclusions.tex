In this work, we looked at the NER problem from three different perspectives, namely, sequence labeling, question answering and span-based approach. We compared and contrasted them with each other and studied their advantages and limitations. Taking inspiration from the QA setup, we proposed the span detection and classification pipeline which uses a reverse question formulation. Additionally, we also proposed to convert from a sparse character space to a dense pattern space through which we can learn meaningful intrinsic character patterns in alphanumeric and pattern-oriented entities. We demonstrated the effectiveness of our proposed domain-agnostic techniques on multiple datasets in general English and biomedical domains. We also presented a study depicting that trivial concatenation of external semantic vectors with BERT outputs may not train the model effectively at lower learning rates.

Our span-based setup opens up prospects for more intuitive and creative ways of approaching the NER problem. However, the pipelined nature of the approach currently serves as a bottleneck. It may be worthwhile to think of some ensemble-based approach where we train individual BERT models on some sub-problems and each of those models contributed its part to solve the overall NER problem in a majority-voting setup.

Our study on training effectiveness reveals that feeding additional external semantics while fine-tuning the BERT model is non-trivial. This again motivates future research on designing feature fusion techniques which are effective with a BERT (transformer-like) architecture.

From the qualitative analysis of the various approaches, we observe that boundary detection serves as a primary issue in NER. To alleviate this problem, we explicitly model word types and special symbols. However, there is still a wide margin to cover. We encourage the research community to design architectures or new training objectives tailor-made to handle mention boundaries effectively. 