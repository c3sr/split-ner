The domain of this thesis revolves around the following broad categories. We look at some works from each category to get a better understanding of each.

\section{Named Entity Recognition}
From a deep learning perspective, one of the most popular models uses a CNN-LSTM-CRF framework proposed by \cite{ma2016end}. The CNN helps capture intrinsic character level information, the bidirectional-LSTM captures token-interrelationships and the CRF helps model the output tag transitions. The model approaches the problem as a sequence tagging task with BIO tagging scheme. This means for \texttt{K} distinct entity types, we have \texttt{2K+1} output classes (\textit{B-Class} and \textit{I-Class} for each and the \textit{NONE} class). \cite{akbik2018contextual} proposes contextualized string embeddings created by passing a sentence character-wise to Bi-LSTM and forming token embeddings by concatenating the boundary character vectors. \cite{li2019unified} instead treats NER as a question answering task where the entity to be extracted is given as a question, followed by the actual sentence, to BERT \cite{devlin2018bert} and the model learns to output the correct entity spans. \cite{li2019dice} develops on top of this and uses Dice Loss objective to further improve the performance. \cite{yamada2020luke} uses a different pretraining objective and propose entity-aware self-attention mechanism. Recently, \cite{luoma2020exploring} use cross-sentence context with BERT model to extract sentence semantics better. As a side note, sometimes gold labels in datasets may have some errors. \cite{wang2019crossweigh} use a bootstrapping framework to correct such imperfect annotations.

\section{BIOMEDICAL NER}
In the science and biomedical domain, NER task has its own set of special challenges. Entity mentions are long and may have lots of alpha-numeric symbols, abbreviations, codes etc. which may be hard for a language model to make sense of. Many entities also are low-resource with a shortage of labelled training data. \cite{wang2019cross} use a multi-task learning framework and combine labelled data from multiple corpora mapping entity tags across corpora to coherent classes. \cite{banerjee2019knowledge} use the MRC framework with dataset concatenation.

\section{LOW-RESOURCE NER}
\cite{zhou2019dual} propose adversarial perturbations and use the CNN-LSTM-CRF architecture to make the model learn low-resource entities better. \cite{arora2017extracting} use regular expression patterns similar to Hearst patterns for artificially generating more training data and train a LSTM model for entity extraction.

\section{NUMERICAL UNDERSTANDING}
In biomedical NER as well as personal information extraction, lots of entities are alphanumeric. Many such as phone numbers, SSNs, look very much alike and require understanding of numbers with patterns to semantically differentiate the two. Dealing with numbers is different from normal text, since the word does not itself have some known semantics like most English words. \cite{chen2019numeracy} learn number semantics using a Bi-GRU frameowrk. \cite{wallace2019nlp} shows that standard contextual word embeddings like ELMo\cite{peters2018deep}, BERT\cite{devlin2018bert} all have some good sense of numbers and found that character-level embeddings work better than word-level embeddings in learning numeral semantics.
