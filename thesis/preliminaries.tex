\section{Task Formulation}
\label{sec:task_formulation}
\textbf{Formal Definition}. Consider $\mathcal{T}$ as a set of entity types that the user is interested in extracting. Given a sentence $\mathcal{S}$ as a $N$-length sequence of tokens, $\mathcal{S} = \langle w_1, w_2 \ldots w_N \rangle$, named entity recognition (NER) is defined as the task of extracting a list of tuples $\langle s, e, t \rangle$ such that $s \in [1, N]$ is the \textit{start} index, $e \in [1, N]$ is the \textit{end} index and $t \in \mathcal{T}$ is the \textit{entity type}. Corresponding to each such tuple, the n-gram, $w_{s\,..\,e}$ is called a \textit{mention} of entity type $t$.

\textbf{Example}. Let $\mathcal{T} = \{$\texttt{Person}, \texttt{Location}$\}$. Consider a tokenized input sentence $\mathcal{S} = \langle$\textit{Emily}, \textit{lives}, \textit{in}, \textit{United}, \textit{States}$\rangle$. NER task is to output tuples $\langle 1, 1,$ \texttt{Person}$\rangle$ and $\langle 4, 5,$ \texttt{Location}$\rangle$. Here, \textit{Emily} is a mention of entity \texttt{Person} and \textit{United States} is a mention of entity \texttt{Location}.

\section{Nature of Entities}
\label{sec:nature_of_entities}
Language dynamics and style of writing varies from domain-to-domain and so does the nature of informing entities in corresponding sentences. This affects the approaches one should use for extracting entities and the expected performance. Some differentiating characteristics of entities are:

\begin{itemize}
    \item \textbf{N-gram length}. Scientific literature include \texttt{chemicals}, \texttt{diseases}, \texttt{phenomena} which are all generally long and wordy while news articles include \texttt{person}, \texttt{organization}, and \texttt{location} which are comparatively smaller. 
    
    \item \textbf{Intrinsic properties}. Common entities in news articles like \texttt{person}, \texttt{location} are wordy and alphabetic. They are called \textbf{word-based entities}. Frequently occurring ones can be learnt or else can be inferred from surrounding context. However there are entities like \texttt{driving license} and \texttt{telephone number} that have semantics encoded in the form of alphanumeric and special symbol patterns (like \texttt{telephone number} has form \texttt{xxx-xxx-xxxx} with a area code in the beginning). Similarly, in biomedical domain, there are \texttt{chemical} and \texttt{gene} abbreviations/formulas like \textit{MgCl2} and \textit{mutCK1delta gene}. Their semantics lies in the intrinsic language of chemistry where \textit{Mg} is \textit{Magnesium} and \textit{Cl} is \textit{Chlorine}. We call these \textbf{non-word entities}.
    
    \item \textbf{Diversity}. Some entities are \textit{broad spectrum} with lots of diverse variety of mentions. For example, \texttt{chemicals} are sometimes written in words like \textit{potassium permanganate} or as formula \textit{KMnO4}, sometimes with intricate special symbols like, \textit{Ca(2+)}. Other entities may be \textit{narrow spectrum} following some distinct patterns, like \texttt{phone numbers}. 
    
    \item \textbf{Hierarchy}. Depending on the use-case, not all entities to be extracted may be at the same level when organized in a concept taxonomy. For example, in computer science literature, we may be interested in extracting mentions related to \texttt{cybersecurity}, \texttt{RNN}, \texttt{CNN}, \texttt{transformers}. Here, \texttt{RNN}, \texttt{CNN}, \texttt{transformers} are all niche entity classes which come under the broad umbrella of machine learning and are at a lower level in hierarchy than \texttt{cybersecurity}.
    
    \item \textbf{Nesting}. Some entity mentions may be embedded within bigger mentions thus developing a nested structure. For example, \textit{Goldman Sachs London} is an \texttt{Organization} while \textit{London} is a \texttt{Location}. Several such examples can be found in the biomedical domain in \texttt{GENIA} corpus\cite{kim2003genia} and news article domain in \texttt{ACE 2004}\cite{mitchell2005ace} and \texttt{ACE 2005}\cite{walker2006ace} corpora.
    
    \item \textbf{Mention density}. This characteristic may have some influence of the \textit{diversity} and \textit{hierarchy} concepts described above. Depending on the domain, not all labeled entities may have an equable representation in the data. Some may have lots of examples. We call such entities as \textbf{high-resource entities} while others may have only a few and are called \textbf{low-resource entities}.
    
\end{itemize}

\section{Evaluation Metrics}
\label{sec:evaluation_metrics}
All the results reported in this work are calculated on predefined test set data unless otherwise specified. We report \textit{micro-averaged F1} score using \textit{exact-match} evaluation over \textit{entity spans}. 

\begin{definition}
\label{def:mention_correctness}
An entity mention tuple $\langle s, e, t \rangle$ extracted by NER system on a given sentence is considered correct iff both the boundary positions, $s$ and $e$, along with the entity type $t$ are correct as per ground truth.
\end{definition} 

Based on Definition \ref{def:mention_correctness}, we count:

\begin{itemize}
    \item True Positives ($TP$): Count of mention tuples returned by NER system which also appear in ground truth.
    
    \item False Positives ($FP$): Count of mention tuples returned by NER system which do \textit{not} appear in ground truth.
    
    \item False Negatives ($FN$): Count of mention tuples which appear in ground truth but are not extracted by the NER system.
\end{itemize}

Precision refers to percentage of retrieved NER results which are correct. Recall refers to the percentage of ground truth entities correctly retrieved by our system. F$_1$ score is the harmonic mean of precision and recall. Note that here we are counting mention tuples as a whole and not separately for each entity type. So the values calculated are known as \textit{micro-averaged} values over all entity types.
\begin{align*}
    \text{Micro-Precision} &= \frac{TP}{TP + FP} & \text{Micro-Recall} &= \frac{TP}{TP + FN}
\end{align*}
\begin{align*}
    \text{Micro-F}_1 &= 2 \times \frac{\text{Micro-Precision} \times \text{Micro-Recall}}{\text{Micro-Precision} + \text{Micro-Recall}}
\end{align*}

\section{Tagging Scheme}
\label{sec:tagging_scheme}
In order to capture the boundaries of entity mentions correctly, it is a standard practice to assign a label to each token as per a tagging scheme. Some tagging schemes popular in NER literature are:

\begin{itemize}
    \item \texttt{SingleTag}: Every token of an entity mention $\langle s, e, \texttt{Tag} \rangle$ is assigned label \texttt{Tag}. All other tokens are assigned a special label \texttt{O}.
    
    \item \texttt{BIO}: For every entity mention of the form $\langle s, e, \texttt{Tag} \rangle$, token at position $s$ is assigned label \texttt{B-Tag} (\texttt{B} stands for \textit{begin}). All other tokens in the mention are assigned \texttt{I-Tag} (\texttt{I} stands for \textit{intermediate}). Remaining tokens, not part of any mention, are assigned \texttt{O} label. 
    
    \item \texttt{BIOE}: Apart from the \texttt{B}, \texttt{I} and \texttt{O} labels in \texttt{BIO} scheme, here the token at position $e$ is assigned \texttt{E-Tag} (\texttt{E} stands for \textit{end}). Unigram mentions are labeled with \texttt{B-Tag}.
    
    \item \texttt{BIOES}: Additionally over \texttt{BIOE} scheme, unigram mentions are labeled \texttt{S-Tag} (\texttt{S} stands for \textit{singleton}).
\end{itemize}

\section{Datasets}
\label{sec:datasets}
In this study we work with multiple English language datasets belonging to news and biomedical domains. Table \ref{tab:datasets_summary} gives a summary of the datasets and nature of entities they posses. We define non-word entities, high/low resource, nesting in Section \ref{sec:nature_of_entities}.

\begin{table}[h!]
	\begin{tabular}{|c|c|c|c|p{6em}|c|}\hline
	\textbf{Dataset} & \textbf{Domain} & \textbf{\#Entities} & \textbf{Non-Word} & \textbf{High/Low Resource} & \textbf{Nesting}\\\hline
	\texttt{CoNLL 2003} & News & 4 & No & High & Flat\\\hline
	\texttt{OntoNotes 5.0} & News & 18 & Yes & High + Low & Flat\\\hline
	\texttt{BioNLP13CG} & Biomedical & 16 & Yes & High + Low & Nested\\\hline
	\texttt{JNLPBA} & Biomedical & 4 & Yes & High & Flat\\\hline
	\end{tabular}
	\caption{Datasets Summary}
	\label{tab:datasets_summary}
\end{table}

\subsection{CoNLL 2003 Dataset}

The \texttt{CoNLL 2003}\cite{sang2003introduction} corpus is a collection of news wire articles from the Reuters corpus manually annotated with 4 entity types: \texttt{PER} (Person), \texttt{ORG} (Organization), \texttt{LOC} (Location) and \texttt{MISC} (Miscellaneous). We obtain the dataset from \texttt{datasets}\footnote{https://huggingface.co/datasets/conll2003} package. Around $16.8\%$ of tokens in the dataset are part of some entity mention. From diversity perspective, the \texttt{MISC} class is more diverse than the other 3 classes. Only $4.6\%$ tokens belonging to entity mentions are non-word. Hence all named entities are predominantly word-based. The corpus has no nesting and as per the distribution of entities shown in Table \ref{tab:conll_entity_distribution}, none of the entities are low-resource. Average sentence length is $14.5$ tokens. Table \ref{tab:conll_dataset_split} shows the \texttt{Train}/\texttt{Dev}/\texttt{Test} splits. 

\begin{table}[h!]
\begin{subtable}[t]{.48\linewidth}
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Entity} & \textbf{Count}\\\hline
	\texttt{LOC} & 10645\\\hline
	\texttt{PER} & 10059\\\hline
	\texttt{ORG} & 9323\\\hline
	\texttt{MISC} & 5062\\\hline
	\end{tabular}
	\caption{Entity Distribution}
	\label{tab:conll_entity_distribution}
% }
\end{subtable}
\begin{subtable}[t]{.48\linewidth}
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Split} & \textbf{\# Sentences}\\\hline
	\texttt{Train} & 14041\\\hline
	\texttt{Dev} & 3250\\\hline
	\texttt{Test} & 3453\\\hline
	\end{tabular}
	\caption{Data Split}
	\label{tab:conll_dataset_split}
\end{subtable}
\caption{CoNLL 2003 Dataset Stats}
\end{table}

\subsection{OntoNotes 5.0 Dataset}

\texttt{OntoNotes 5.0}\cite{weischedel2013ontonotes} is a large corpus consisting of text from various genres (news, weblogs, talk shows, broadcast, and conversational
telephone speech) labeled with structural information (syntax) and shallow semantics (word sense linked to an ontology and coreference). There are
5 versions, from Release 1.0 to Release 5.0. We work with Release 5.0 and focus on named entity labels on English data. There are 18 entity types including common ones like \texttt{PERSON}, \texttt{ORG} and entities having numerical semantics like \texttt{DATE} and \texttt{MONEY}. We obtain the dataset from GitHub\footnote{https://github.com/yuchenlin/OntoNotes-5.0-NER-BIO}. Around $11.0\%$ of tokens in the dataset are part of some entity mention. There are some entities like \texttt{GPE} (geo-political entity) and \texttt{LOCATION} which have very subtle difference in their semantics making them challenging to disambiguate. Corpus is flat-labeled and around $19.3\%$  entity-mention tokens are non-word. As per Table \ref{tab:onto_entity_distribution}, entities like \texttt{PERSON}, \texttt{DATE} are high-resource while \texttt{LANGUAGE}, \texttt{LAW} are low-resource. Average sentence length is $19.0$ tokens. Table \ref{tab:onto_dataset_split} shows the \texttt{Train}/\texttt{Dev}/\texttt{Test} splits. 

\begin{table}[h!]
\begin{subtable}[t]{.48\linewidth}
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Entity} & \textbf{Count}\\\hline
	\texttt{ORG} & 29963\\\hline
    \texttt{GPE} & 28133\\\hline
    \texttt{PERSON} & 27332\\\hline
    \texttt{DATE} & 23786\\\hline
    \texttt{CARDINAL} & 13626\\\hline
    \texttt{NORP} & 11608\\\hline
    \texttt{MONEY} & 6425\\\hline
    \texttt{PERCENT} & 4866\\\hline
    \texttt{ORDINAL} & 2737\\\hline
    \texttt{LOC} & 2691\\\hline
    \texttt{TIME} & 2289\\\hline
    \texttt{WORK\_OF\_ART} & 1650\\\hline
    \texttt{QUANTITY} & 1583\\\hline
    \texttt{FAC} & 1440\\\hline
    \texttt{PRODUCT} & 1296\\\hline
    \texttt{EVENT} & 1273\\\hline
    \texttt{LAW} & 568\\\hline
    \texttt{LANGUAGE} & 412\\\hline
	\end{tabular}
	\caption{Entity Distribution}
	\label{tab:onto_entity_distribution}
% }
\end{subtable}
\begin{subtable}[t]{.48\linewidth}
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Split} & \textbf{\# Sentences}\\\hline
	\texttt{Train} & 115812\\\hline
	\texttt{Dev} & 15680\\\hline
	\texttt{Test} & 12217\\\hline
	\end{tabular}
	\caption{Data Split}
	\label{tab:onto_dataset_split}
\end{subtable}
\caption{OntoNotes 5.0 English NER Dataset Stats}
\end{table}

\subsection{JNLPBA Dataset}

The \texttt{JNLPBA}\cite{kim2004introduction} dataset comes from GENIA\cite{kim2003genia} corpus (version 3.2) and contains abstracts of papers taken from MEDLINE database. The GENIA corpus consists of 36 fine-grained nested named entity types. For preparing JNLPBA, some of these entity classes are combined to a higher-level entity class and others are ignored. In all, JNLPBA has 5 flat-labeled entity types: \texttt{protein}, \texttt{DNA}, \texttt{RNA}, \texttt{cell\_line}, \texttt{cell\_type}. We obtain the dataset available from GitHub\footnote{https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data/JNLPBA}. Around $21.7\%$ of tokens in the dataset are part of some entity mention. Around $31.1\%$ of entity mention tokens are non-word. As per Table \ref{tab:jnlpba_entity_distribution}, most entities are high resource although representation of \texttt{RNA} is comparatively less. Average sentence length is $26.5$ tokens. Table \ref{tab:jnlpba_dataset_split} shows the \texttt{Train}/\texttt{Dev}/\texttt{Test} splits. 

\begin{table}[h!]
\begin{subtable}[t]{.48\linewidth}
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Entity} & \textbf{Count}\\\hline
	\texttt{protein} & 35336\\\hline
    \texttt{DNA} & 10589\\\hline
    \texttt{cell\_type} & 8639\\\hline
    \texttt{cell\_line} & 4330\\\hline
    \texttt{RNA} & 1069\\\hline
	\end{tabular}
	\caption{Entity Distribution}
	\label{tab:jnlpba_entity_distribution}
% }
\end{subtable}
\begin{subtable}[t]{.48\linewidth}
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Split} & \textbf{\# Sentences}\\\hline
	\texttt{Train} & 16807\\\hline
	\texttt{Dev} & 1739\\\hline
	\texttt{Test} & 3856\\\hline
	\end{tabular}
	\caption{Data Split}
	\label{tab:jnlpba_dataset_split}
\end{subtable}
\caption{JNLPBA Dataset Stats}
\end{table}

\subsection{BioNLP13CG Dataset}

The \texttt{BioNLP13CG}\cite{pyysalo2015overview} (Cancer Genetics) dataset comes from BioNLP Shared Task 2013. The text belongs to the theme of biological processes relating to the development and progression of cancer. It consists of 16 entity types with a mix of high-resource and low-resource ones. We obtain the dataset available on the shared task website\footnote{http://2013.bionlp-st.org/tasks/cancer-genetics} and process it into \texttt{tsv} format. For most part of NER study when using this dataset we focus on flat-annotated entity mentions and ignore the small percentage (around $1\%$) of nested entities both from training and evaluation. The flat-annotated corpus is consistent with the one available on GitHub\footnote{https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data/BioNLP13CG-IOB}. Around $23.5\%$ of tokens in the dataset are part of some entity mention. Around $24.9\%$ of entity mention tokens are non-word. Average sentence length is $27.6$ tokens. Table \ref{tab:bio_entity_distribution} shows the distribution of entities and Table \ref{tab:bio_dataset_split} gives the \texttt{Train}/\texttt{Dev}/\texttt{Test} splits. 

\begin{table}[h!]
\begin{subtable}[t]{.48\linewidth}
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Entity} & \textbf{Count}\\\hline
	\texttt{Gene\_or\_gene\_product} & 7908\\\hline
    \texttt{Cell} & 3492\\\hline
    \texttt{Cancer} & 2582\\\hline
    \texttt{Simple\_chemical} & 2270\\\hline
    \texttt{Organism} & 1715\\\hline
    \texttt{Multi-tissue\_structure} & 857\\\hline
    \texttt{Tissue} & 587\\\hline
    \texttt{Cellular\_component} & 569\\\hline
    \texttt{Organ} & 421\\\hline
    \texttt{Organism\_substance} & 283\\\hline
    \texttt{Pathological\_formation} & 228\\\hline
    \texttt{Amino\_acid} & 135\\\hline
    \texttt{Immaterial\_anatomical\_entity} & 102\\\hline
    \texttt{Organism\_subdivision} & 98\\\hline
    \texttt{Anatomical\_system} & 41\\\hline
    \texttt{Developing\_anatomical\_structure} & 35\\\hline
	\end{tabular}
	\caption{Entity Distribution}
	\label{tab:bio_entity_distribution}
% }
\end{subtable}
\begin{subtable}[t]{.48\linewidth}
\centering
\begin{tabular}{|c|c|}\hline
	\textbf{Split} & \textbf{\# Sentences}\\\hline
	\texttt{Train} & 3033\\\hline
	\texttt{Dev} & 1003\\\hline
	\texttt{Test} & 1906\\\hline
	\end{tabular}
	\caption{Data Split}
	\label{tab:bio_dataset_split}
\end{subtable}
\caption{BioNLP13CG Dataset Stats}
\end{table}

\section{Experimental Setup}
We report all our results on the test sets after taking the model checkpoint corresponding to the best micro-averaged F1-score on development set. The development set evaluation takes place at steps of 0.5 training epochs. We train the models for $300$ epochs at learning rate $10^{-5}$ unless otherwise specified. 

We use \texttt{transformers}\footnote{https://github.com/huggingface/transformers} python library by HuggingFace and \texttt{pytorch} for implementation and fix random seed to $42$ for replication. For general English corpora like \texttt{CoNLL 2003} and \texttt{OntoNotes 5.0}, by default, we use the pretrained \texttt{bert-base-uncased}\footnote{https://huggingface.co/bert-base-uncased} model. For biomedical datasets, \texttt{BioNLP13CG} and \texttt{JNLPBA}, we use \texttt{BioBERT-Base}\footnote{https://github.com/dmis-lab/biobert\#download} model. Note that in all our experiments, we only use the BERT-Base architecture which has around 110M trainable parameters. We use \texttt{Nvidia GeForce GTX 1080} and \texttt{Nvidia Tesla V100} gpus for model training and evaluation.

We use cross entropy loss during training unless otherwise specified. The training data is randomly shuffled and a batch size of $16$ is used with post-padding. For BERT-based models, we fix maximum sequence length to $256$ for \texttt{BioNLP13CG}, \texttt{CoNLL 2003}, \texttt{JNLPBA} datasets and $512$ for \texttt{OntoNotes 5.0} data. Unless otherwise specified, the BERT-based models output entity labels for each sub-token (as per WordPiece tokenization) of an existing token in the dataset. As a heuristic, we take the label of first sub-token as the label for the corresponding token during our evaluations.
