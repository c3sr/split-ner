Named Entity Recognition (NER) is the task of extracting informing entities belonging to predefined semantic classes from raw text. These semantic classes could be general purpose like person, location or domain-specific like genes, protein names in biomedical texts. NER has widespread applications in natural language processing (NLP) and serves as the foundation for applications like question answering, information retrieval, machine translation etc. Recently, the NER task has got a lot of traction in the research community with the advent of deep learning models like CNN-LSTM-CRF, BERT etc. which are able to capture textual semantics very well.

In this work, we present a detailed NER study approaching the task from three perspectives, namely, sequence labeling, question answering (QA), and span detection and classification. We propose a simple span detection and classification setup that first detects all mention spans without entity type and later questions the model to assign the mention to an output entity type. This setup is reverse of traditional QA-based NER where we feed the entity type as input and expect mention spans as output. We also introduce explicit pattern embeddings which help compliment character embeddings even with less training data. Experimental results demonstrate the effectiveness of our proposed domain-agnostic techniques on multiple datasets. We set the new state-of-the-art for \texttt{BioNLP13CG} and give competitive performance on \texttt{CoNLL 2003} and \texttt{JNLPBA} datasets. Additionally, we probe into the BERT model and show that mere concatenation of external feature vectors with BERT outputs may not train effectively at the recommended low learning rates for BERT.